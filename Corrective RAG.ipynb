{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583b581d",
   "metadata": {},
   "source": [
    "# 🔧 Corrective RAG (CRAG) Implementation\n",
    "\n",
    "## Self-Correcting AI Travel Assistant with Real-Time Verification\n",
    "\n",
    "This notebook demonstrates a **Corrective RAG system** with:\n",
    "- 🔍 Intelligent Document Retrieval\n",
    "- 📊 Confidence Evaluation & Quality Assessment\n",
    "- 🌐 Real-Time Web Search Integration\n",
    "- 🧩 Query Decomposition & Re-retrieval\n",
    "- 🔄 Knowledge Base Refinement\n",
    "- ⚡ Adaptive Correction Mechanisms\n",
    "\n",
    "### Key Benefits of Corrective RAG\n",
    "- **Self-Correcting**: Automatically detects and fixes poor retrievals\n",
    "- **Real-Time Updates**: Verifies information through web search\n",
    "- **Adaptive**: Adjusts strategy based on confidence levels\n",
    "- **Reliable**: Ensures high-quality, up-to-date responses\n",
    "\n",
    "### Use Case: Smart Travel Planning Assistant\n",
    "Perfect for travel applications where information freshness and accuracy are critical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb02f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: google-generativeai in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (0.8.5)\n",
      "Requirement already satisfied: rank-bm25 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: transformers in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (4.52.4)\n",
      "Requirement already satisfied: scikit-learn in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: python-dotenv in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: requests in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (4.13.4)\n",
      "Collecting dateparser\n",
      "  Using cached dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: tqdm in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scipy in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: packaging in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: pydantic in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-generativeai) (2.11.7)\n",
      "Requirement already satisfied: google-api-python-client in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-generativeai) (2.173.0)\n",
      "Requirement already satisfied: protobuf in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: google-api-core in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from requests) (2025.6.15)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from beautifulsoup4) (2.7)\n",
      "Collecting tzlocal>=0.2\n",
      "  Using cached tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from dateparser) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2024.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from dateparser) (2025.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-api-core->google-generativeai) (1.73.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-api-core->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: jinja2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: networkx in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from triton==3.3.1->torch>=1.11.0->sentence-transformers) (58.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Installing collected packages: tzlocal, dateparser\n",
      "Successfully installed dateparser-1.2.1 tzlocal-5.3.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install sentence-transformers faiss-cpu google-generativeai rank-bm25 transformers scikit-learn numpy python-dotenv requests beautifulsoup4 dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555c6cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohdasimkhan/.pyenv/versions/3.10.2/envs/rags/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import requests\n",
    "from typing import List, Dict, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass, field\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "from datetime import datetime, timedelta\n",
    "import dateparser\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import google.generativeai as genai\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "print(\"📚 Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d77a628",
   "metadata": {},
   "source": [
    "## 🏗️ Core Data Structures\n",
    "\n",
    "Define the foundational structures for our CRAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d50ce4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ Data structures defined!\n"
     ]
    }
   ],
   "source": [
    "# Enums for system configuration\n",
    "class ConfidenceLevel(Enum):\n",
    "    HIGH = \"high\"          # Use retrieved documents directly\n",
    "    MEDIUM = \"medium\"      # Decompose query and re-retrieve\n",
    "    LOW = \"low\"            # Trigger web search or knowledge refinement\n",
    "\n",
    "class CorrectionAction(Enum):\n",
    "    NONE = \"none\"                    # No correction needed\n",
    "    DECOMPOSE_QUERY = \"decompose\"    # Break down query\n",
    "    WEB_SEARCH = \"web_search\"        # Search web for current info\n",
    "    REFINE_KNOWLEDGE = \"refine\"      # Update knowledge base\n",
    "    HYBRID_SEARCH = \"hybrid\"         # Multiple strategies\n",
    "\n",
    "class TravelDomain(Enum):\n",
    "    HOTELS = \"hotels\"\n",
    "    FLIGHTS = \"flights\"\n",
    "    RESTAURANTS = \"restaurants\"\n",
    "    ATTRACTIONS = \"attractions\"\n",
    "    TRANSPORTATION = \"transportation\"\n",
    "    GENERAL = \"general\"\n",
    "\n",
    "class RetrievalStrategy(Enum):\n",
    "    SEMANTIC = \"semantic\"\n",
    "    KEYWORD = \"keyword\"\n",
    "    HYBRID = \"hybrid\"\n",
    "\n",
    "# Core data structures\n",
    "@dataclass\n",
    "class TravelDocument:\n",
    "    id: str\n",
    "    title: str\n",
    "    content: str\n",
    "    domain: TravelDomain\n",
    "    location: str\n",
    "    last_updated: datetime\n",
    "    keywords: List[str] = field(default_factory=list)\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "    confidence_indicators: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class TravelQuery:\n",
    "    id: str\n",
    "    text: str\n",
    "    domain: TravelDomain\n",
    "    location: Optional[str] = None\n",
    "    date_range: Optional[Tuple[datetime, datetime]] = None\n",
    "    user_id: Optional[str] = None\n",
    "    session_id: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    document: TravelDocument\n",
    "    score: float\n",
    "    rank: int\n",
    "    strategy_used: RetrievalStrategy\n",
    "    relevance_indicators: Dict[str, float] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class ConfidenceAssessment:\n",
    "    level: ConfidenceLevel\n",
    "    score: float\n",
    "    reasons: List[str]\n",
    "    recommended_action: CorrectionAction\n",
    "    indicators: Dict[str, float] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class CorrectionResult:\n",
    "    action_taken: CorrectionAction\n",
    "    new_documents: List[RetrievalResult]\n",
    "    success: bool\n",
    "    processing_time: float\n",
    "    details: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class CRAGResponse:\n",
    "    query: TravelQuery\n",
    "    initial_retrieval: List[RetrievalResult]\n",
    "    confidence_assessment: ConfidenceAssessment\n",
    "    correction_result: Optional[CorrectionResult]\n",
    "    final_documents: List[RetrievalResult]\n",
    "    generated_answer: str\n",
    "    overall_confidence: float\n",
    "    processing_pipeline: List[str]\n",
    "    processing_time: float\n",
    "    correction_applied: bool\n",
    "\n",
    "print(\"🏗️ Data structures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d22f530",
   "metadata": {},
   "source": [
    "## 📚 Travel Knowledge Base\n",
    "\n",
    "Create a comprehensive travel knowledge base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d79b6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Travel knowledge base created with 8 documents\n",
      "🌍 Domains: {'attractions', 'general', 'restaurants', 'flights', 'transportation', 'hotels'}\n",
      "📍 Locations: {'Bangkok, Thailand', 'Bangkok to Tokyo'}\n"
     ]
    }
   ],
   "source": [
    "# Travel knowledge base with various freshness levels\n",
    "travel_knowledge_base = [\n",
    "    {\n",
    "        \"id\": \"hotel_001\",\n",
    "        \"title\": \"Grand Palace Hotel Bangkok - Luxury Accommodation\",\n",
    "        \"domain\": \"hotels\",\n",
    "        \"location\": \"Bangkok, Thailand\",\n",
    "        \"content\": \"The Grand Palace Hotel Bangkok offers luxury accommodation in the heart of Bangkok. Features include 24-hour room service, fitness center, spa, outdoor pool, and multiple dining options. Located near major attractions like the Grand Palace and Wat Pho temple. Rooms range from deluxe to presidential suites with city or river views. Average rate: $150-400 per night. Check-in: 3 PM, Check-out: 12 PM.\",\n",
    "        \"last_updated\": datetime(2024, 1, 15),\n",
    "        \"keywords\": [\"hotel\", \"luxury\", \"Bangkok\", \"palace\", \"spa\", \"pool\"],\n",
    "        \"confidence_indicators\": {\"price_freshness\": 0.6, \"availability_freshness\": 0.3}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"flight_001\",\n",
    "        \"title\": \"Bangkok to Tokyo Flight Routes - Airlines and Schedules\",\n",
    "        \"domain\": \"flights\",\n",
    "        \"location\": \"Bangkok to Tokyo\",\n",
    "        \"content\": \"Multiple airlines operate between Bangkok (BKK) and Tokyo (NRT/HND). Thai Airways, ANA, JAL, and budget carriers like Scoot offer direct flights. Flight duration: 6-7 hours. Typical prices: $300-800 economy, $1200-2500 business class. Peak seasons: March-May, July-August, December. Book 2-3 months in advance for better rates. Check visa requirements for Thailand and Japan.\",\n",
    "        \"last_updated\": datetime(2023, 12, 10),\n",
    "        \"keywords\": [\"flight\", \"Bangkok\", \"Tokyo\", \"airlines\", \"schedule\", \"price\"],\n",
    "        \"confidence_indicators\": {\"price_freshness\": 0.4, \"schedule_freshness\": 0.5}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"restaurant_001\",\n",
    "        \"title\": \"Gaggan Restaurant Bangkok - Progressive Indian Cuisine\",\n",
    "        \"domain\": \"restaurants\",\n",
    "        \"location\": \"Bangkok, Thailand\",\n",
    "        \"content\": \"Gaggan is a world-renowned progressive Indian restaurant in Bangkok, previously ranked #1 in Asia's 50 Best Restaurants. Chef Gaggan Anand creates innovative molecular gastronomy interpretations of Indian cuisine. Tasting menu: $200-300 per person. Reservations essential, book 2-3 months ahead. Open Tuesday-Sunday, closed Mondays. Located in a beautiful colonial house with garden seating.\",\n",
    "        \"last_updated\": datetime(2024, 2, 1),\n",
    "        \"keywords\": [\"restaurant\", \"Gaggan\", \"Bangkok\", \"Indian\", \"fine dining\", \"molecular\"],\n",
    "        \"confidence_indicators\": {\"menu_freshness\": 0.8, \"price_freshness\": 0.7}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"attraction_001\",\n",
    "        \"title\": \"Wat Pho Temple - Temple of the Reclining Buddha\",\n",
    "        \"domain\": \"attractions\",\n",
    "        \"location\": \"Bangkok, Thailand\",\n",
    "        \"content\": \"Wat Pho is one of Bangkok's oldest and most important temples, famous for the 46-meter long Reclining Buddha statue. The temple complex houses over 1,000 Buddha images and is considered the first public university in Thailand. Traditional Thai massage school operates here. Entry fee: 200 THB for foreigners. Open daily 8 AM - 6:30 PM. Dress code: Cover shoulders and knees. Allow 2-3 hours for visit.\",\n",
    "        \"last_updated\": datetime(2024, 3, 5),\n",
    "        \"keywords\": [\"temple\", \"Wat Pho\", \"Bangkok\", \"Buddha\", \"massage\", \"attraction\"],\n",
    "        \"confidence_indicators\": {\"price_freshness\": 0.9, \"hours_freshness\": 0.8}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"transport_001\",\n",
    "        \"title\": \"Bangkok Public Transportation - BTS, MRT, and Taxis\",\n",
    "        \"domain\": \"transportation\",\n",
    "        \"location\": \"Bangkok, Thailand\",\n",
    "        \"content\": \"Bangkok offers various transportation options: BTS Skytrain (elevated rail), MRT subway, buses, taxis, and tuk-tuks. BTS/MRT: 16-52 THB per trip, operates 6 AM - midnight. Taxis: meter starts at 35 THB, traffic can be heavy. Grab ride-hailing app widely available. Airport Rail Link connects Suvarnabhumi Airport to city center (45 THB). Consider getting a Rabbit Card for BTS/MRT convenience.\",\n",
    "        \"last_updated\": datetime(2023, 11, 20),\n",
    "        \"keywords\": [\"transportation\", \"BTS\", \"MRT\", \"taxi\", \"Bangkok\", \"public transport\"],\n",
    "        \"confidence_indicators\": {\"price_freshness\": 0.5, \"schedule_freshness\": 0.6}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"hotel_002\",\n",
    "        \"title\": \"Budget Hostels in Khao San Road - Backpacker Haven\",\n",
    "        \"domain\": \"hotels\",\n",
    "        \"location\": \"Bangkok, Thailand\",\n",
    "        \"content\": \"Khao San Road is Bangkok's famous backpacker street with numerous budget accommodations. Hostels offer dorm beds ($8-15) and private rooms ($20-40). Popular options include Mad Monkey Hostel, Lub d Bangkok Siam, and NapPark Hostel. Most include free WiFi, common areas, and tour booking services. Area is lively with street food, bars, and shops. Can be noisy - bring earplugs. Book ahead during peak season.\",\n",
    "        \"last_updated\": datetime(2024, 1, 8),\n",
    "        \"keywords\": [\"hostel\", \"budget\", \"Khao San Road\", \"backpacker\", \"Bangkok\"],\n",
    "        \"confidence_indicators\": {\"price_freshness\": 0.7, \"availability_freshness\": 0.4}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"food_001\",\n",
    "        \"title\": \"Bangkok Street Food Guide - Must-Try Local Dishes\",\n",
    "        \"domain\": \"restaurants\",\n",
    "        \"location\": \"Bangkok, Thailand\",\n",
    "        \"content\": \"Bangkok street food offers incredible variety and flavors. Must-try dishes: Pad Thai (60-80 THB), Som Tam (green papaya salad, 40-60 THB), Mango Sticky Rice (80-120 THB), Tom Yum soup (80-150 THB). Best street food areas: Chatuchak Weekend Market, Chinatown, Khao San Road. Food courts in malls offer air-conditioned dining with similar prices. Always choose busy stalls with high turnover for freshness.\",\n",
    "        \"last_updated\": datetime(2024, 2, 20),\n",
    "        \"keywords\": [\"street food\", \"Bangkok\", \"Pad Thai\", \"local cuisine\", \"markets\"],\n",
    "        \"confidence_indicators\": {\"price_freshness\": 0.8, \"location_freshness\": 0.9}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"weather_001\",\n",
    "        \"title\": \"Bangkok Weather and Best Time to Visit\",\n",
    "        \"domain\": \"general\",\n",
    "        \"location\": \"Bangkok, Thailand\",\n",
    "        \"content\": \"Bangkok has a tropical climate with three seasons: Cool (Nov-Feb): 20-30°C, dry and pleasant, peak tourist season. Hot (Mar-May): 30-35°C, very hot and humid. Rainy (Jun-Oct): 25-32°C, daily afternoon showers, fewer crowds, lower prices. Best time to visit: November to February for comfortable weather. Pack light, breathable clothing, umbrella during rainy season. Air conditioning is essential in hotels.\",\n",
    "        \"last_updated\": datetime(2023, 10, 15),\n",
    "        \"keywords\": [\"weather\", \"Bangkok\", \"climate\", \"seasons\", \"best time\"],\n",
    "        \"confidence_indicators\": {\"seasonal_accuracy\": 0.9, \"current_conditions\": 0.3}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"📚 Travel knowledge base created with {len(travel_knowledge_base)} documents\")\n",
    "print(f\"🌍 Domains: {set(doc['domain'] for doc in travel_knowledge_base)}\")\n",
    "print(f\"📍 Locations: {set(doc['location'] for doc in travel_knowledge_base)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ccc582",
   "metadata": {},
   "source": [
    "## 🧩 Base Module Classes\n",
    "\n",
    "Define abstract base classes for CRAG architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0188c924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 Base CRAG module class defined!\n"
     ]
    }
   ],
   "source": [
    "# Base module class for CRAG\n",
    "class CRAGModule(ABC):\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.call_count = 0\n",
    "        self.success_count = 0\n",
    "        self.created_at = datetime.now()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def process(self, input_data: Any) -> Any:\n",
    "        pass\n",
    "    \n",
    "    def update_stats(self, success: bool = True):\n",
    "        self.call_count += 1\n",
    "        if success:\n",
    "            self.success_count += 1\n",
    "    \n",
    "    def get_info(self):\n",
    "        success_rate = (self.success_count / max(self.call_count, 1)) * 100\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'calls': self.call_count,\n",
    "            'success_rate': success_rate,\n",
    "            'created': self.created_at\n",
    "        }\n",
    "\n",
    "print(\"🧩 Base CRAG module class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb12b6e3",
   "metadata": {},
   "source": [
    "## 🔍 Query Processing Module\n",
    "\n",
    "Intelligent travel query understanding and classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3806366e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Test query processed:\n",
      "   Text: Find luxury hotels in Bangkok for next week\n",
      "   Domain: hotels\n",
      "   Location: Bangkok\n",
      "✅ Travel Query Module ready!\n"
     ]
    }
   ],
   "source": [
    "class TravelQueryModule(CRAGModule):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"TravelQueryModule\")\n",
    "        \n",
    "        # Domain detection patterns\n",
    "        self.domain_patterns = {\n",
    "            TravelDomain.HOTELS: [\"hotel\", \"accommodation\", \"stay\", \"room\", \"booking\", \"lodge\"],\n",
    "            TravelDomain.FLIGHTS: [\"flight\", \"airline\", \"airport\", \"fly\", \"ticket\", \"aviation\"],\n",
    "            TravelDomain.RESTAURANTS: [\"restaurant\", \"food\", \"dining\", \"eat\", \"cuisine\", \"meal\"],\n",
    "            TravelDomain.ATTRACTIONS: [\"attraction\", \"temple\", \"museum\", \"sightseeing\", \"tour\", \"visit\"],\n",
    "            TravelDomain.TRANSPORTATION: [\"transport\", \"taxi\", \"bus\", \"train\", \"metro\", \"bts\", \"mrt\"],\n",
    "        }\n",
    "        \n",
    "        # Location patterns\n",
    "        self.location_patterns = [\n",
    "            r\"in ([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\",\n",
    "            r\"at ([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\",\n",
    "            r\"([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*) (?:hotel|restaurant|attraction)\"\n",
    "        ]\n",
    "    \n",
    "    def process(self, query_text: str) -> TravelQuery:\n",
    "        self.update_stats()\n",
    "        \n",
    "        query_id = str(uuid.uuid4())[:8]\n",
    "        domain = self._detect_domain(query_text)\n",
    "        location = self._extract_location(query_text)\n",
    "        date_range = self._extract_dates(query_text)\n",
    "        \n",
    "        return TravelQuery(\n",
    "            id=query_id,\n",
    "            text=query_text,\n",
    "            domain=domain,\n",
    "            location=location,\n",
    "            date_range=date_range\n",
    "        )\n",
    "    \n",
    "    def _detect_domain(self, text: str) -> TravelDomain:\n",
    "        text_lower = text.lower()\n",
    "        domain_scores = {}\n",
    "        \n",
    "        for domain, keywords in self.domain_patterns.items():\n",
    "            score = sum(1 for keyword in keywords if keyword in text_lower)\n",
    "            domain_scores[domain] = score\n",
    "        \n",
    "        if max(domain_scores.values()) > 0:\n",
    "            return max(domain_scores, key=domain_scores.get)\n",
    "        return TravelDomain.GENERAL\n",
    "    \n",
    "    def _extract_location(self, text: str) -> Optional[str]:\n",
    "        for pattern in self.location_patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def _extract_dates(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n",
    "        # Simple date extraction - can be enhanced\n",
    "        date_patterns = [\n",
    "            r\"(\\d{1,2}/\\d{1,2}/\\d{4})\",\n",
    "            r\"(\\d{4}-\\d{2}-\\d{2})\",\n",
    "            r\"(next week|next month|tomorrow)\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in date_patterns:\n",
    "            matches = re.findall(pattern, text.lower())\n",
    "            if matches:\n",
    "                try:\n",
    "                    parsed_date = dateparser.parse(matches[0])\n",
    "                    if parsed_date:\n",
    "                        end_date = parsed_date + timedelta(days=7)  # Default 7-day trip\n",
    "                        return (parsed_date, end_date)\n",
    "                except:\n",
    "                    pass\n",
    "        return None\n",
    "\n",
    "# Test travel query module\n",
    "travel_query_module = TravelQueryModule()\n",
    "test_query = travel_query_module.process(\"Find luxury hotels in Bangkok for next week\")\n",
    "print(f\"🔍 Test query processed:\")\n",
    "print(f\"   Text: {test_query.text}\")\n",
    "print(f\"   Domain: {test_query.domain.value}\")\n",
    "print(f\"   Location: {test_query.location}\")\n",
    "print(\"✅ Travel Query Module ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ab4a75",
   "metadata": {},
   "source": [
    "## 🔎 Retrieval Module\n",
    "\n",
    "Document retrieval with travel-specific optimizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef4e541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Travel retrieval module initialized\n",
      "📚 Indexing 8 travel documents...\n",
      "✅ Travel documents indexed successfully!\n",
      "✅ Travel Retrieval Module ready!\n"
     ]
    }
   ],
   "source": [
    "class TravelRetrievalModule(CRAGModule):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"TravelRetrievalModule\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.documents = []\n",
    "        self.semantic_index = None\n",
    "        self.bm25_index = None\n",
    "        \n",
    "        print(\"🔎 Travel retrieval module initialized\")\n",
    "    \n",
    "    def index_documents(self, documents: List[Dict]):\n",
    "        print(f\"📚 Indexing {len(documents)} travel documents...\")\n",
    "        \n",
    "        # Convert to TravelDocument objects\n",
    "        self.documents = [\n",
    "            TravelDocument(\n",
    "                id=doc['id'],\n",
    "                title=doc['title'],\n",
    "                content=doc['content'],\n",
    "                domain=TravelDomain(doc['domain']),\n",
    "                location=doc['location'],\n",
    "                last_updated=doc['last_updated'],\n",
    "                keywords=doc.get('keywords', []),\n",
    "                confidence_indicators=doc.get('confidence_indicators', {})\n",
    "            )\n",
    "            for doc in documents\n",
    "        ]\n",
    "        \n",
    "        # Build semantic index\n",
    "        self._build_semantic_index()\n",
    "        \n",
    "        # Build keyword index\n",
    "        self._build_keyword_index()\n",
    "        \n",
    "        print(\"✅ Travel documents indexed successfully!\")\n",
    "    \n",
    "    def _build_semantic_index(self):\n",
    "        doc_texts = [f\"{doc.title} {doc.content} {doc.location}\" for doc in self.documents]\n",
    "        embeddings = self.embedding_model.encode(doc_texts)\n",
    "        \n",
    "        # Store embeddings\n",
    "        for doc, embedding in zip(self.documents, embeddings):\n",
    "            doc.embedding = embedding\n",
    "        \n",
    "        # Create FAISS index\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.semantic_index = faiss.IndexFlatIP(dimension)\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.semantic_index.add(embeddings.astype('float32'))\n",
    "    \n",
    "    def _build_keyword_index(self):\n",
    "        doc_texts = [f\"{doc.title} {doc.content} {doc.location}\" for doc in self.documents]\n",
    "        tokenized_docs = [text.lower().split() for text in doc_texts]\n",
    "        self.bm25_index = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    def process(self, query: TravelQuery, strategy: RetrievalStrategy = RetrievalStrategy.HYBRID, top_k: int = 5) -> List[RetrievalResult]:\n",
    "        self.update_stats()\n",
    "        \n",
    "        if strategy == RetrievalStrategy.SEMANTIC:\n",
    "            results = self._semantic_search(query, top_k)\n",
    "        elif strategy == RetrievalStrategy.KEYWORD:\n",
    "            results = self._keyword_search(query, top_k)\n",
    "        else:  # HYBRID\n",
    "            results = self._hybrid_search(query, top_k)\n",
    "        \n",
    "        # Apply travel-specific filtering\n",
    "        return self._apply_travel_filters(query, results)\n",
    "    \n",
    "    def _semantic_search(self, query: TravelQuery, top_k: int) -> List[RetrievalResult]:\n",
    "        search_text = f\"{query.text} {query.location or ''}\"\n",
    "        query_embedding = self.embedding_model.encode([search_text])\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        scores, indices = self.semantic_index.search(query_embedding.astype('float32'), top_k * 2)\n",
    "        \n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            if idx < len(self.documents):  # Valid index check\n",
    "                relevance_indicators = self._calculate_relevance_indicators(query, self.documents[idx], float(score))\n",
    "                results.append(RetrievalResult(\n",
    "                    document=self.documents[idx],\n",
    "                    score=float(score),\n",
    "                    rank=i + 1,\n",
    "                    strategy_used=RetrievalStrategy.SEMANTIC,\n",
    "                    relevance_indicators=relevance_indicators\n",
    "                ))\n",
    "        return results[:top_k]\n",
    "    \n",
    "    def _keyword_search(self, query: TravelQuery, top_k: int) -> List[RetrievalResult]:\n",
    "        search_text = f\"{query.text} {query.location or ''}\"\n",
    "        query_tokens = search_text.lower().split()\n",
    "        scores = self.bm25_index.get_scores(query_tokens)\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            relevance_indicators = self._calculate_relevance_indicators(query, self.documents[idx], float(scores[idx]))\n",
    "            results.append(RetrievalResult(\n",
    "                document=self.documents[idx],\n",
    "                score=float(scores[idx]),\n",
    "                rank=i + 1,\n",
    "                strategy_used=RetrievalStrategy.KEYWORD,\n",
    "                relevance_indicators=relevance_indicators\n",
    "            ))\n",
    "        return results\n",
    "    \n",
    "    def _hybrid_search(self, query: TravelQuery, top_k: int) -> List[RetrievalResult]:\n",
    "        # Get results from both strategies\n",
    "        semantic_results = self._semantic_search(query, top_k * 2)\n",
    "        keyword_results = self._keyword_search(query, top_k * 2)\n",
    "        \n",
    "        # Normalize scores\n",
    "        self._normalize_scores(semantic_results)\n",
    "        self._normalize_scores(keyword_results)\n",
    "        \n",
    "        # Combine with travel-aware weights\n",
    "        semantic_weight = 0.7\n",
    "        keyword_weight = 0.3\n",
    "        \n",
    "        combined_scores = {}\n",
    "        \n",
    "        for result in semantic_results:\n",
    "            doc_id = result.document.id\n",
    "            combined_scores[doc_id] = {\n",
    "                'document': result.document,\n",
    "                'semantic_score': result.score,\n",
    "                'keyword_score': 0.0,\n",
    "                'relevance_indicators': result.relevance_indicators\n",
    "            }\n",
    "        \n",
    "        for result in keyword_results:\n",
    "            doc_id = result.document.id\n",
    "            if doc_id in combined_scores:\n",
    "                combined_scores[doc_id]['keyword_score'] = result.score\n",
    "            else:\n",
    "                combined_scores[doc_id] = {\n",
    "                    'document': result.document,\n",
    "                    'semantic_score': 0.0,\n",
    "                    'keyword_score': result.score,\n",
    "                    'relevance_indicators': result.relevance_indicators\n",
    "                }\n",
    "        \n",
    "        # Calculate final scores\n",
    "        final_results = []\n",
    "        for doc_id, scores in combined_scores.items():\n",
    "            final_score = (semantic_weight * scores['semantic_score'] + \n",
    "                          keyword_weight * scores['keyword_score'])\n",
    "            \n",
    "            final_results.append(RetrievalResult(\n",
    "                document=scores['document'],\n",
    "                score=final_score,\n",
    "                rank=0,\n",
    "                strategy_used=RetrievalStrategy.HYBRID,\n",
    "                relevance_indicators=scores['relevance_indicators']\n",
    "            ))\n",
    "        \n",
    "        # Sort and assign ranks\n",
    "        final_results.sort(key=lambda x: x.score, reverse=True)\n",
    "        for i, result in enumerate(final_results[:top_k]):\n",
    "            result.rank = i + 1\n",
    "        \n",
    "        return final_results[:top_k]\n",
    "    \n",
    "    def _calculate_relevance_indicators(self, query: TravelQuery, document: TravelDocument, base_score: float) -> Dict[str, float]:\n",
    "        indicators = {\n",
    "            'base_score': base_score,\n",
    "            'domain_match': 1.0 if query.domain == document.domain else 0.5,\n",
    "            'location_match': 0.0,\n",
    "            'freshness_score': self._calculate_freshness_score(document),\n",
    "            'keyword_overlap': self._calculate_keyword_overlap(query, document)\n",
    "        }\n",
    "        \n",
    "        # Location matching\n",
    "        if query.location and document.location:\n",
    "            if query.location.lower() in document.location.lower():\n",
    "                indicators['location_match'] = 1.0\n",
    "            elif any(word in document.location.lower() for word in query.location.lower().split()):\n",
    "                indicators['location_match'] = 0.7\n",
    "        \n",
    "        return indicators\n",
    "    \n",
    "    def _calculate_freshness_score(self, document: TravelDocument) -> float:\n",
    "        days_old = (datetime.now() - document.last_updated).days\n",
    "        \n",
    "        # Different domains have different freshness requirements\n",
    "        if document.domain in [TravelDomain.FLIGHTS, TravelDomain.HOTELS]:\n",
    "            # Pricing info becomes stale quickly\n",
    "            return max(0.0, 1.0 - days_old / 90)  # 90 days for full decay\n",
    "        elif document.domain == TravelDomain.RESTAURANTS:\n",
    "            # Menu and pricing moderate freshness needs\n",
    "            return max(0.0, 1.0 - days_old / 180)  # 180 days\n",
    "        else:\n",
    "            # Attractions and general info stay fresh longer\n",
    "            return max(0.0, 1.0 - days_old / 365)  # 365 days\n",
    "    \n",
    "    def _calculate_keyword_overlap(self, query: TravelQuery, document: TravelDocument) -> float:\n",
    "        query_words = set(query.text.lower().split())\n",
    "        doc_words = set((document.title + \" \" + document.content).lower().split())\n",
    "        \n",
    "        if not query_words:\n",
    "            return 0.0\n",
    "        \n",
    "        overlap = len(query_words.intersection(doc_words))\n",
    "        return overlap / len(query_words)\n",
    "    \n",
    "    def _apply_travel_filters(self, query: TravelQuery, results: List[RetrievalResult]) -> List[RetrievalResult]:\n",
    "        # Apply travel-specific filtering and boosting\n",
    "        filtered_results = []\n",
    "        \n",
    "        for result in results:\n",
    "            # Boost domain matches\n",
    "            if query.domain == result.document.domain:\n",
    "                result.score *= 1.2\n",
    "            \n",
    "            # Boost location matches\n",
    "            if result.relevance_indicators.get('location_match', 0) > 0.8:\n",
    "                result.score *= 1.15\n",
    "            \n",
    "            # Apply freshness penalty for time-sensitive domains\n",
    "            freshness = result.relevance_indicators.get('freshness_score', 1.0)\n",
    "            if result.document.domain in [TravelDomain.FLIGHTS, TravelDomain.HOTELS] and freshness < 0.5:\n",
    "                result.score *= 0.8  # Penalty for stale pricing info\n",
    "            \n",
    "            filtered_results.append(result)\n",
    "        \n",
    "        # Re-sort by adjusted scores\n",
    "        filtered_results.sort(key=lambda x: x.score, reverse=True)\n",
    "        for i, result in enumerate(filtered_results):\n",
    "            result.rank = i + 1\n",
    "        \n",
    "        return filtered_results\n",
    "    \n",
    "    def _normalize_scores(self, results: List[RetrievalResult]):\n",
    "        if not results:\n",
    "            return\n",
    "        \n",
    "        scores = [result.score for result in results]\n",
    "        min_score, max_score = min(scores), max(scores)\n",
    "        \n",
    "        if max_score > min_score:\n",
    "            for result in results:\n",
    "                result.score = (result.score - min_score) / (max_score - min_score)\n",
    "\n",
    "# Initialize travel retrieval module\n",
    "travel_retrieval_module = TravelRetrievalModule()\n",
    "travel_retrieval_module.index_documents(travel_knowledge_base)\n",
    "\n",
    "print(\"✅ Travel Retrieval Module ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ffe98",
   "metadata": {},
   "source": [
    "## 📊 Confidence Assessment Module\n",
    "\n",
    "The core of CRAG - evaluating retrieval quality and determining correction needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f9446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Confidence Assessment Module ready!\n"
     ]
    }
   ],
   "source": [
    "class ConfidenceAssessmentModule(CRAGModule):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"ConfidenceAssessmentModule\")\n",
    "        \n",
    "        # Confidence thresholds\n",
    "        self.thresholds = {\n",
    "            'high_confidence': 0.75,\n",
    "            'medium_confidence': 0.45,\n",
    "            'min_documents': 2,\n",
    "            'freshness_threshold': 0.6\n",
    "        }\n",
    "        \n",
    "        # Domain-specific confidence requirements\n",
    "        self.domain_requirements = {\n",
    "            TravelDomain.FLIGHTS: {'freshness_weight': 0.4, 'accuracy_weight': 0.6},\n",
    "            TravelDomain.HOTELS: {'freshness_weight': 0.3, 'accuracy_weight': 0.7},\n",
    "            TravelDomain.RESTAURANTS: {'freshness_weight': 0.25, 'accuracy_weight': 0.75},\n",
    "            TravelDomain.ATTRACTIONS: {'freshness_weight': 0.1, 'accuracy_weight': 0.9},\n",
    "            TravelDomain.TRANSPORTATION: {'freshness_weight': 0.3, 'accuracy_weight': 0.7},\n",
    "            TravelDomain.GENERAL: {'freshness_weight': 0.2, 'accuracy_weight': 0.8}\n",
    "        }\n",
    "    \n",
    "    def process(self, query: TravelQuery, retrieval_results: List[RetrievalResult]) -> ConfidenceAssessment:\n",
    "        self.update_stats()\n",
    "        \n",
    "        # Calculate multiple confidence indicators\n",
    "        indicators = self._calculate_confidence_indicators(query, retrieval_results)\n",
    "        \n",
    "        # Determine overall confidence level\n",
    "        confidence_score = self._calculate_overall_confidence(query, indicators)\n",
    "        confidence_level = self._determine_confidence_level(confidence_score)\n",
    "        \n",
    "        # Generate reasons and recommended action\n",
    "        reasons = self._generate_confidence_reasons(indicators, confidence_level)\n",
    "        recommended_action = self._recommend_correction_action(query, confidence_level, indicators)\n",
    "        \n",
    "        return ConfidenceAssessment(\n",
    "            level=confidence_level,\n",
    "            score=confidence_score,\n",
    "            reasons=reasons,\n",
    "            recommended_action=recommended_action,\n",
    "            indicators=indicators\n",
    "        )\n",
    "    \n",
    "    def _calculate_confidence_indicators(self, query: TravelQuery, results: List[RetrievalResult]) -> Dict[str, float]:\n",
    "        if not results:\n",
    "            return {\n",
    "                'retrieval_quality': 0.0,\n",
    "                'result_count_score': 0.0,\n",
    "                'freshness_score': 0.0,\n",
    "                'relevance_score': 0.0,\n",
    "                'domain_match_score': 0.0,\n",
    "                'location_match_score': 0.0,\n",
    "                'score_variance': 0.0\n",
    "            }\n",
    "        \n",
    "        indicators = {}\n",
    "        \n",
    "        # 1. Retrieval Quality (top scores)\n",
    "        top_scores = [r.score for r in results[:3]]\n",
    "        indicators['retrieval_quality'] = np.mean(top_scores) if top_scores else 0.0\n",
    "        \n",
    "        # 2. Result Count Score\n",
    "        indicators['result_count_score'] = min(1.0, len(results) / 5.0)\n",
    "        \n",
    "        # 3. Freshness Score\n",
    "        freshness_scores = [r.relevance_indicators.get('freshness_score', 0.5) for r in results]\n",
    "        indicators['freshness_score'] = np.mean(freshness_scores)\n",
    "        \n",
    "        # 4. Relevance Score (based on domain and location matches)\n",
    "        domain_matches = [r.relevance_indicators.get('domain_match', 0.5) for r in results]\n",
    "        location_matches = [r.relevance_indicators.get('location_match', 0.0) for r in results]\n",
    "        indicators['domain_match_score'] = np.mean(domain_matches)\n",
    "        indicators['location_match_score'] = np.mean(location_matches)\n",
    "        indicators['relevance_score'] = (indicators['domain_match_score'] + indicators['location_match_score']) / 2\n",
    "        \n",
    "        # 5. Score Variance (consistency indicator)\n",
    "        scores = [r.score for r in results]\n",
    "        indicators['score_variance'] = 1.0 - (np.std(scores) / (np.mean(scores) + 1e-6)) if len(scores) > 1 else 1.0\n",
    "        indicators['score_variance'] = max(0.0, min(1.0, indicators['score_variance']))\n",
    "        \n",
    "        # 6. Keyword Overlap Score\n",
    "        keyword_overlaps = [r.relevance_indicators.get('keyword_overlap', 0.0) for r in results]\n",
    "        indicators['keyword_overlap_score'] = np.mean(keyword_overlaps)\n",
    "        \n",
    "        return indicators\n",
    "    \n",
    "    def _calculate_overall_confidence(self, query: TravelQuery, indicators: Dict[str, float]) -> float:\n",
    "        # Get domain-specific weights\n",
    "        domain_req = self.domain_requirements.get(query.domain, self.domain_requirements[TravelDomain.GENERAL])\n",
    "        \n",
    "        # Base confidence calculation\n",
    "        base_confidence = (\n",
    "            indicators['retrieval_quality'] * 0.25 +\n",
    "            indicators['relevance_score'] * 0.25 +\n",
    "            indicators['result_count_score'] * 0.15 +\n",
    "            indicators['score_variance'] * 0.15 +\n",
    "            indicators['keyword_overlap_score'] * 0.20\n",
    "        )\n",
    "        \n",
    "        # Apply domain-specific adjustments\n",
    "        freshness_impact = indicators['freshness_score'] * domain_req['freshness_weight']\n",
    "        accuracy_impact = base_confidence * domain_req['accuracy_weight']\n",
    "        \n",
    "        final_confidence = accuracy_impact + freshness_impact\n",
    "        \n",
    "        # Apply penalties for critical issues\n",
    "        if indicators['result_count_score'] < 0.4:  # Very few results\n",
    "            final_confidence *= 0.7\n",
    "        \n",
    "        if query.domain in [TravelDomain.FLIGHTS, TravelDomain.HOTELS] and indicators['freshness_score'] < 0.3:\n",
    "            final_confidence *= 0.6  # Heavy penalty for stale pricing info\n",
    "        \n",
    "        return max(0.0, min(1.0, final_confidence))\n",
    "    \n",
    "    def _determine_confidence_level(self, confidence_score: float) -> ConfidenceLevel:\n",
    "        if confidence_score >= self.thresholds['high_confidence']:\n",
    "            return ConfidenceLevel.HIGH\n",
    "        elif confidence_score >= self.thresholds['medium_confidence']:\n",
    "            return ConfidenceLevel.MEDIUM\n",
    "        else:\n",
    "            return ConfidenceLevel.LOW\n",
    "    \n",
    "    def _generate_confidence_reasons(self, indicators: Dict[str, float], level: ConfidenceLevel) -> List[str]:\n",
    "        reasons = []\n",
    "        \n",
    "        if level == ConfidenceLevel.HIGH:\n",
    "            reasons.append(f\"High retrieval quality (score: {indicators['retrieval_quality']:.2f})\")\n",
    "            if indicators['relevance_score'] > 0.8:\n",
    "                reasons.append(\"Strong domain and location matching\")\n",
    "            if indicators['freshness_score'] > 0.7:\n",
    "                reasons.append(\"Recent and fresh information\")\n",
    "        \n",
    "        elif level == ConfidenceLevel.MEDIUM:\n",
    "            reasons.append(f\"Moderate confidence (score: {indicators['retrieval_quality']:.2f})\")\n",
    "            if indicators['result_count_score'] < 0.6:\n",
    "                reasons.append(\"Limited number of relevant documents\")\n",
    "            if indicators['freshness_score'] < 0.6:\n",
    "                reasons.append(\"Some information may be outdated\")\n",
    "            if indicators['relevance_score'] < 0.7:\n",
    "                reasons.append(\"Partial domain or location matching\")\n",
    "        \n",
    "        else:  # LOW\n",
    "            reasons.append(f\"Low confidence (score: {indicators['retrieval_quality']:.2f})\")\n",
    "            if indicators['result_count_score'] < 0.4:\n",
    "                reasons.append(\"Very few relevant documents found\")\n",
    "            if indicators['freshness_score'] < 0.3:\n",
    "                reasons.append(\"Information appears outdated\")\n",
    "            if indicators['relevance_score'] < 0.5:\n",
    "                reasons.append(\"Poor domain or location matching\")\n",
    "            if indicators['keyword_overlap_score'] < 0.3:\n",
    "                reasons.append(\"Low keyword overlap with query\")\n",
    "        \n",
    "        return reasons\n",
    "    \n",
    "    def _recommend_correction_action(self, query: TravelQuery, level: ConfidenceLevel, indicators: Dict[str, float]) -> CorrectionAction:\n",
    "        if level == ConfidenceLevel.HIGH:\n",
    "            return CorrectionAction.NONE\n",
    "        \n",
    "        elif level == ConfidenceLevel.MEDIUM:\n",
    "            # For medium confidence, try query decomposition first\n",
    "            if len(query.text.split()) > 8:  # Complex query\n",
    "                return CorrectionAction.DECOMPOSE_QUERY\n",
    "            elif indicators['freshness_score'] < 0.5 and query.domain in [TravelDomain.FLIGHTS, TravelDomain.HOTELS]:\n",
    "                return CorrectionAction.WEB_SEARCH  # Price-sensitive domains need fresh data\n",
    "            else:\n",
    "                return CorrectionAction.HYBRID_SEARCH\n",
    "        \n",
    "        else:  # LOW confidence\n",
    "            # For low confidence, more aggressive correction\n",
    "            if indicators['freshness_score'] < 0.3:\n",
    "                return CorrectionAction.WEB_SEARCH  # Definitely need fresh data\n",
    "            elif indicators['result_count_score'] < 0.3:\n",
    "                return CorrectionAction.DECOMPOSE_QUERY  # Try breaking down the query\n",
    "            else:\n",
    "                return CorrectionAction.REFINE_KNOWLEDGE  # Knowledge base issue\n",
    "\n",
    "# Test confidence assessment\n",
    "confidence_module = ConfidenceAssessmentModule()\n",
    "print(\"✅ Confidence Assessment Module ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9626d02",
   "metadata": {},
   "source": [
    "## 🔧 Correction Action Modules\n",
    "\n",
    "Modules that perform corrective actions based on confidence assessment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b274b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Correction Action Modules ready!\n"
     ]
    }
   ],
   "source": [
    "class WebSearchModule(CRAGModule):\n",
    "    \"\"\"Web search module for real-time information verification\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"WebSearchModule\")\n",
    "        self.search_timeout = 10\n",
    "    \n",
    "    def process(self, query: TravelQuery) -> CorrectionResult:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Create search queries based on travel domain\n",
    "            search_queries = self._create_search_queries(query)\n",
    "            \n",
    "            # Simulate web search (in real implementation, use actual search API)\n",
    "            web_results = self._simulate_web_search(search_queries)\n",
    "            \n",
    "            # Convert web results to retrieval results\n",
    "            new_documents = self._convert_web_results(web_results, query)\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            self.update_stats(True)\n",
    "            return CorrectionResult(\n",
    "                action_taken=CorrectionAction.WEB_SEARCH,\n",
    "                new_documents=new_documents,\n",
    "                success=True,\n",
    "                processing_time=processing_time,\n",
    "                details={'search_queries': search_queries, 'results_found': len(web_results)}\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            processing_time = time.time() - start_time\n",
    "            self.update_stats(False)\n",
    "            return CorrectionResult(\n",
    "                action_taken=CorrectionAction.WEB_SEARCH,\n",
    "                new_documents=[],\n",
    "                success=False,\n",
    "                processing_time=processing_time,\n",
    "                details={'error': str(e)}\n",
    "            )\n",
    "    \n",
    "    def _create_search_queries(self, query: TravelQuery) -> List[str]:\n",
    "        base_query = query.text\n",
    "        location = query.location or \"\"\n",
    "        \n",
    "        search_queries = []\n",
    "        \n",
    "        if query.domain == TravelDomain.HOTELS:\n",
    "            search_queries.extend([\n",
    "                f\"{base_query} {location} prices 2024\",\n",
    "                f\"{location} hotel booking rates current\",\n",
    "                f\"{base_query} {location} availability\"\n",
    "            ])\n",
    "        elif query.domain == TravelDomain.FLIGHTS:\n",
    "            search_queries.extend([\n",
    "                f\"{base_query} flight prices current\",\n",
    "                f\"{location} flight schedules 2024\",\n",
    "                f\"{base_query} airline deals\"\n",
    "            ])\n",
    "        elif query.domain == TravelDomain.RESTAURANTS:\n",
    "            search_queries.extend([\n",
    "                f\"{base_query} {location} menu prices 2024\",\n",
    "                f\"{location} restaurant reviews recent\",\n",
    "                f\"{base_query} {location} opening hours\"\n",
    "            ])\n",
    "        else:\n",
    "            search_queries.append(f\"{base_query} {location} 2024 current\")\n",
    "        \n",
    "        return search_queries\n",
    "    \n",
    "    def _simulate_web_search(self, search_queries: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Simulate web search results - replace with actual search API in production\"\"\"\n",
    "        simulated_results = [\n",
    "            {\n",
    "                'title': 'Bangkok Hotels - Current Prices and Availability',\n",
    "                'content': 'Updated hotel prices for Bangkok: Grand Palace Hotel currently $180-420/night, availability good for next month. New promotion: 15% off for bookings made this week. Peak season rates apply Dec-Feb.',\n",
    "                'url': 'https://example-booking.com/bangkok-hotels',\n",
    "                'last_updated': datetime.now(),\n",
    "                'relevance_score': 0.9\n",
    "            },\n",
    "            {\n",
    "                'title': 'Bangkok Flight Deals - Real-time Pricing',\n",
    "                'content': 'Live flight prices to Bangkok: Economy from $350, Business from $1400. New route announcements from AirAsia and Thai Airways. Current fuel surcharges and seasonal adjustments applied.',\n",
    "                'url': 'https://example-flights.com/bangkok',\n",
    "                'last_updated': datetime.now(),\n",
    "                'relevance_score': 0.85\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Simulate processing delay\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        return simulated_results\n",
    "    \n",
    "    def _convert_web_results(self, web_results: List[Dict], query: TravelQuery) -> List[RetrievalResult]:\n",
    "        \"\"\"Convert web search results to RetrievalResult objects\"\"\"\n",
    "        retrieval_results = []\n",
    "        \n",
    "        for i, result in enumerate(web_results):\n",
    "            # Create a temporary document from web result\n",
    "            web_doc = TravelDocument(\n",
    "                id=f\"web_{uuid.uuid4().hex[:8]}\",\n",
    "                title=result['title'],\n",
    "                content=result['content'],\n",
    "                domain=query.domain,\n",
    "                location=query.location or \"Unknown\",\n",
    "                last_updated=result['last_updated'],\n",
    "                keywords=[],\n",
    "                confidence_indicators={'freshness_score': 1.0, 'web_verified': True}\n",
    "            )\n",
    "            \n",
    "            retrieval_results.append(RetrievalResult(\n",
    "                document=web_doc,\n",
    "                score=result['relevance_score'],\n",
    "                rank=i + 1,\n",
    "                strategy_used=RetrievalStrategy.HYBRID,\n",
    "                relevance_indicators={\n",
    "                    'freshness_score': 1.0,\n",
    "                    'web_verified': True,\n",
    "                    'domain_match': 1.0 if query.domain == web_doc.domain else 0.5\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        return retrieval_results\n",
    "\n",
    "\n",
    "class QueryDecompositionModule(CRAGModule):\n",
    "    \"\"\"Breaks down complex queries into simpler sub-queries\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"QueryDecompositionModule\")\n",
    "    \n",
    "    def process(self, query: TravelQuery, retrieval_module: TravelRetrievalModule) -> CorrectionResult:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Decompose query into sub-queries\n",
    "            sub_queries = self._decompose_query(query)\n",
    "            \n",
    "            # Retrieve documents for each sub-query\n",
    "            all_results = []\n",
    "            for sub_query in sub_queries:\n",
    "                sub_results = retrieval_module.process(sub_query, RetrievalStrategy.HYBRID, top_k=3)\n",
    "                all_results.extend(sub_results)\n",
    "            \n",
    "            # Remove duplicates and re-rank\n",
    "            unique_results = self._deduplicate_and_rerank(all_results)\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            self.update_stats(True)\n",
    "            return CorrectionResult(\n",
    "                action_taken=CorrectionAction.DECOMPOSE_QUERY,\n",
    "                new_documents=unique_results,\n",
    "                success=True,\n",
    "                processing_time=processing_time,\n",
    "                details={'sub_queries': [sq.text for sq in sub_queries], 'total_results': len(all_results)}\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            processing_time = time.time() - start_time\n",
    "            self.update_stats(False)\n",
    "            return CorrectionResult(\n",
    "                action_taken=CorrectionAction.DECOMPOSE_QUERY,\n",
    "                new_documents=[],\n",
    "                success=False,\n",
    "                processing_time=processing_time,\n",
    "                details={'error': str(e)}\n",
    "            )\n",
    "    \n",
    "    def _decompose_query(self, query: TravelQuery) -> List[TravelQuery]:\n",
    "        \"\"\"Decompose complex query into simpler sub-queries\"\"\"\n",
    "        sub_queries = []\n",
    "        text = query.text.lower()\n",
    "        \n",
    "        # Pattern-based decomposition\n",
    "        if \"and\" in text:\n",
    "            parts = text.split(\" and \")\n",
    "            for part in parts:\n",
    "                sub_queries.append(TravelQuery(\n",
    "                    id=f\"{query.id}_sub_{len(sub_queries)}\",\n",
    "                    text=part.strip(),\n",
    "                    domain=query.domain,\n",
    "                    location=query.location,\n",
    "                    date_range=query.date_range\n",
    "                ))\n",
    "        \n",
    "        # Domain-specific decomposition\n",
    "        elif query.domain == TravelDomain.HOTELS:\n",
    "            if \"luxury\" in text and \"bangkok\" in text:\n",
    "                sub_queries.extend([\n",
    "                    TravelQuery(f\"{query.id}_sub_0\", \"luxury hotels\", query.domain, query.location),\n",
    "                    TravelQuery(f\"{query.id}_sub_1\", f\"hotels {query.location}\", query.domain, query.location),\n",
    "                    TravelQuery(f\"{query.id}_sub_2\", \"hotel prices\", query.domain, query.location)\n",
    "                ])\n",
    "        \n",
    "        # If no decomposition patterns match, create topic-based sub-queries\n",
    "        if not sub_queries:\n",
    "            keywords = text.split()\n",
    "            if len(keywords) > 4:\n",
    "                mid = len(keywords) // 2\n",
    "                sub_queries.extend([\n",
    "                    TravelQuery(f\"{query.id}_sub_0\", \" \".join(keywords[:mid]), query.domain, query.location),\n",
    "                    TravelQuery(f\"{query.id}_sub_1\", \" \".join(keywords[mid:]), query.domain, query.location)\n",
    "                ])\n",
    "            else:\n",
    "                # Add domain-specific context\n",
    "                sub_queries.append(TravelQuery(\n",
    "                    f\"{query.id}_sub_0\", \n",
    "                    f\"{text} {query.domain.value}\", \n",
    "                    query.domain, \n",
    "                    query.location\n",
    "                ))\n",
    "        \n",
    "        return sub_queries or [query]  # Return original if no decomposition\n",
    "    \n",
    "    def _deduplicate_and_rerank(self, results: List[RetrievalResult]) -> List[RetrievalResult]:\n",
    "        \"\"\"Remove duplicate documents and re-rank by relevance\"\"\"\n",
    "        seen_docs = set()\n",
    "        unique_results = []\n",
    "        \n",
    "        # Sort by score first\n",
    "        results.sort(key=lambda x: x.score, reverse=True)\n",
    "        \n",
    "        for result in results:\n",
    "            doc_id = result.document.id\n",
    "            if doc_id not in seen_docs:\n",
    "                seen_docs.add(doc_id)\n",
    "                unique_results.append(result)\n",
    "        \n",
    "        # Re-assign ranks\n",
    "        for i, result in enumerate(unique_results):\n",
    "            result.rank = i + 1\n",
    "        \n",
    "        return unique_results[:7]  # Return top 7 unique results\n",
    "\n",
    "\n",
    "class KnowledgeRefinementModule(CRAGModule):\n",
    "    \"\"\"Refines and updates knowledge base based on correction needs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"KnowledgeRefinementModule\")\n",
    "    \n",
    "    def process(self, query: TravelQuery, retrieval_module: TravelRetrievalModule) -> CorrectionResult:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Apply different retrieval strategies\n",
    "            semantic_results = retrieval_module.process(query, RetrievalStrategy.SEMANTIC, top_k=5)\n",
    "            keyword_results = retrieval_module.process(query, RetrievalStrategy.KEYWORD, top_k=5)\n",
    "            \n",
    "            # Combine and enhance results\n",
    "            enhanced_results = self._enhance_results(semantic_results, keyword_results)\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            self.update_stats(True)\n",
    "            return CorrectionResult(\n",
    "                action_taken=CorrectionAction.REFINE_KNOWLEDGE,\n",
    "                new_documents=enhanced_results,\n",
    "                success=True,\n",
    "                processing_time=processing_time,\n",
    "                details={\n",
    "                    'semantic_results': len(semantic_results),\n",
    "                    'keyword_results': len(keyword_results),\n",
    "                    'enhanced_count': len(enhanced_results)\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            processing_time = time.time() - start_time\n",
    "            self.update_stats(False)\n",
    "            return CorrectionResult(\n",
    "                action_taken=CorrectionAction.REFINE_KNOWLEDGE,\n",
    "                new_documents=[],\n",
    "                success=False,\n",
    "                processing_time=processing_time,\n",
    "                details={'error': str(e)}\n",
    "            )\n",
    "    \n",
    "    def _enhance_results(self, semantic_results: List[RetrievalResult], keyword_results: List[RetrievalResult]) -> List[RetrievalResult]:\n",
    "        \"\"\"Enhance results by combining different retrieval strategies\"\"\"\n",
    "        all_results = semantic_results + keyword_results\n",
    "        \n",
    "        # Remove duplicates while preserving best scores\n",
    "        doc_scores = {}\n",
    "        for result in all_results:\n",
    "            doc_id = result.document.id\n",
    "            if doc_id not in doc_scores or result.score > doc_scores[doc_id].score:\n",
    "                doc_scores[doc_id] = result\n",
    "        \n",
    "        # Enhance scores based on multiple strategy agreement\n",
    "        enhanced_results = list(doc_scores.values())\n",
    "        \n",
    "        # Boost scores for documents that appear in both strategies\n",
    "        semantic_doc_ids = {r.document.id for r in semantic_results}\n",
    "        keyword_doc_ids = {r.document.id for r in keyword_results}\n",
    "        \n",
    "        for result in enhanced_results:\n",
    "            if result.document.id in semantic_doc_ids and result.document.id in keyword_doc_ids:\n",
    "                result.score *= 1.2  # Boost for multi-strategy agreement\n",
    "                result.relevance_indicators['multi_strategy_match'] = True\n",
    "        \n",
    "        # Sort by enhanced scores\n",
    "        enhanced_results.sort(key=lambda x: x.score, reverse=True)\n",
    "        \n",
    "        # Re-assign ranks\n",
    "        for i, result in enumerate(enhanced_results):\n",
    "            result.rank = i + 1\n",
    "        \n",
    "        return enhanced_results[:5]\n",
    "\n",
    "\n",
    "# Initialize correction modules\n",
    "web_search_module = WebSearchModule()\n",
    "query_decomposition_module = QueryDecompositionModule()\n",
    "knowledge_refinement_module = KnowledgeRefinementModule()\n",
    "\n",
    "print(\"✅ Correction Action Modules ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09bb012",
   "metadata": {},
   "source": [
    "## 🤖 Adaptive Generation Module\n",
    "\n",
    "Context-aware answer generation with correction awareness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edd0124f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Gemini API configured for CRAG\n",
      "✅ CRAG Generation Module ready!\n"
     ]
    }
   ],
   "source": [
    "class CRAGGenerationModule(CRAGModule):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"CRAGGenerationModule\")\n",
    "        \n",
    "        # Try to initialize Gemini\n",
    "        api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if api_key:\n",
    "            try:\n",
    "                genai.configure(api_key=api_key)\n",
    "                self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "                self.has_llm = True\n",
    "                print(\"🤖 Gemini API configured for CRAG\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Gemini error: {e}\")\n",
    "                self.has_llm = False\n",
    "        else:\n",
    "            print(\"⚠️ No Gemini API key. Using template generation.\")\n",
    "            self.has_llm = False\n",
    "    \n",
    "    def process(self, query: TravelQuery, final_documents: List[RetrievalResult], \n",
    "                confidence_assessment: ConfidenceAssessment, \n",
    "                correction_applied: bool) -> Tuple[str, float]:\n",
    "        self.update_stats()\n",
    "        \n",
    "        if self.has_llm:\n",
    "            return self._generate_with_llm(query, final_documents, confidence_assessment, correction_applied)\n",
    "        else:\n",
    "            return self._generate_with_template(query, final_documents, confidence_assessment, correction_applied)\n",
    "    \n",
    "    def _generate_with_llm(self, query: TravelQuery, documents: List[RetrievalResult], \n",
    "                          confidence: ConfidenceAssessment, correction_applied: bool) -> Tuple[str, float]:\n",
    "        # Prepare context\n",
    "        context_parts = []\n",
    "        for result in documents:\n",
    "            doc = result.document\n",
    "            freshness_info = f\"(Last updated: {doc.last_updated.strftime('%Y-%m-%d')})\"\n",
    "            web_verified = \"✓ Web-verified\" if result.relevance_indicators.get('web_verified', False) else \"\"\n",
    "            context_parts.append(f\"Title: {doc.title}\\nLocation: {doc.location}\\nContent: {doc.content} {freshness_info} {web_verified}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Create CRAG-aware prompt\n",
    "        correction_note = \"\"\n",
    "        if correction_applied:\n",
    "            correction_note = f\"\\n\\nNote: This response includes corrected information. Confidence was initially {confidence.level.value}, so corrective action was taken to improve accuracy.\"\n",
    "        \n",
    "        prompt = f\"\"\"You are a professional travel assistant with access to verified information. \n",
    "Use the provided context to answer the travel query accurately and helpfully.\n",
    "\n",
    "Context (includes verification status):\n",
    "{context}\n",
    "\n",
    "Travel Query: {query.text}\n",
    "Location Focus: {query.location or 'Not specified'}\n",
    "Domain: {query.domain.value}\n",
    "Confidence Level: {confidence.level.value} ({confidence.score:.2f})\n",
    "{correction_note}\n",
    "\n",
    "Instructions:\n",
    "- Provide practical, actionable travel advice\n",
    "- Include current pricing when available\n",
    "- Mention any booking or timing recommendations\n",
    "- Note if information has been recently verified\n",
    "- Be specific about locations and logistics\n",
    "\n",
    "Travel Assistant Response:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            \n",
    "            # Calculate confidence based on correction status and initial confidence\n",
    "            final_confidence = confidence.score\n",
    "            if correction_applied:\n",
    "                final_confidence = min(0.95, confidence.score + 0.15)  # Boost confidence after correction\n",
    "            \n",
    "            return response.text, final_confidence\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Sorry, I encountered an error generating your travel advice: {str(e)}\", 0.0\n",
    "    \n",
    "    def _generate_with_template(self, query: TravelQuery, documents: List[RetrievalResult], \n",
    "                               confidence: ConfidenceAssessment, correction_applied: bool) -> Tuple[str, float]:\n",
    "        if not documents:\n",
    "            return \"I couldn't find relevant travel information to answer your question.\", 0.1\n",
    "        \n",
    "        # Create template-based travel response\n",
    "        answer = f\"Based on {'verified and corrected' if correction_applied else 'available'} travel information for {query.domain.value}\"\n",
    "        if query.location:\n",
    "            answer += f\" in {query.location}\"\n",
    "        answer += \":\\n\\n\"\n",
    "        \n",
    "        for i, result in enumerate(documents[:3], 1):\n",
    "            doc = result.document\n",
    "            freshness = \"(Recently verified)\" if result.relevance_indicators.get('web_verified', False) else f\"(Updated: {doc.last_updated.strftime('%Y-%m-%d')})\"\n",
    "            \n",
    "            answer += f\"**{i}. {doc.title}** {freshness}\\n\"\n",
    "            answer += f\"{doc.content[:300]}{'...' if len(doc.content) > 300 else ''}\\n\\n\"\n",
    "        \n",
    "        if correction_applied:\n",
    "            answer += f\"\\n*Note: This information has been enhanced through corrective search to ensure accuracy and freshness.*\"\n",
    "        \n",
    "        # Adjust confidence based on correction and document count\n",
    "        base_confidence = min(0.8, len(documents) / 3.0)\n",
    "        if correction_applied:\n",
    "            base_confidence = min(0.9, base_confidence + 0.15)\n",
    "        \n",
    "        return answer, base_confidence\n",
    "\n",
    "# Initialize CRAG generation module\n",
    "crag_generation_module = CRAGGenerationModule()\n",
    "print(\"✅ CRAG Generation Module ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891747fc",
   "metadata": {},
   "source": [
    "## 🧩 Complete CRAG System\n",
    "\n",
    "Integrate all modules into the complete Corrective RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "452a46b8-4f66-43a4-ab7b-d39c7276f3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Initializing Corrective RAG (CRAG) System...\n",
      "✅ CRAG System initialized!\n",
      "🔧 Active modules: ['query', 'retrieval', 'confidence', 'web_search', 'decomposition', 'refinement', 'generation']\n",
      "\n",
      "🚀 Complete Corrective RAG System ready!\n"
     ]
    }
   ],
   "source": [
    "class CorrectiveRAGSystem:\n",
    "    def __init__(self):\n",
    "        print(\"🔧 Initializing Corrective RAG (CRAG) System...\")\n",
    "        \n",
    "        # Initialize all modules\n",
    "        self.modules = {\n",
    "            'query': travel_query_module,\n",
    "            'retrieval': travel_retrieval_module,\n",
    "            'confidence': confidence_module,\n",
    "            'web_search': web_search_module,\n",
    "            'decomposition': query_decomposition_module,\n",
    "            'refinement': knowledge_refinement_module,\n",
    "            'generation': crag_generation_module\n",
    "        }\n",
    "        \n",
    "        # System metrics\n",
    "        self.total_queries = 0\n",
    "        self.corrections_applied = 0\n",
    "        self.avg_confidence_improvement = 0.0\n",
    "        self.processing_times = []\n",
    "        \n",
    "        print(\"✅ CRAG System initialized!\")\n",
    "        print(f\"🔧 Active modules: {list(self.modules.keys())}\")\n",
    "    \n",
    "    def process_query(self, query_text: str, user_id: str = None) -> CRAGResponse:\n",
    "        start_time = time.time()\n",
    "        pipeline = []\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\n🔧 CRAG Processing: '{query_text}'\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            # Step 1: Query Processing\n",
    "            print(\"📝 Step 1: Travel query analysis...\")\n",
    "            query = self.modules['query'].process(query_text)\n",
    "            query.user_id = user_id\n",
    "            pipeline.append('query_processing')\n",
    "            \n",
    "            print(f\"   Domain: {query.domain.value}, Location: {query.location or 'Not specified'}\")\n",
    "            \n",
    "            # Step 2: Initial Retrieval\n",
    "            print(\"🔍 Step 2: Initial document retrieval...\")\n",
    "            initial_results = self.modules['retrieval'].process(query, RetrievalStrategy.HYBRID, top_k=5)\n",
    "            pipeline.append('initial_retrieval')\n",
    "            \n",
    "            print(f\"   Retrieved {len(initial_results)} documents\")\n",
    "            for i, result in enumerate(initial_results[:3], 1):\n",
    "                freshness = result.relevance_indicators.get('freshness_score', 0.5)\n",
    "                print(f\"   {i}. {result.document.title} (Score: {result.score:.3f}, Freshness: {freshness:.2f})\")\n",
    "            \n",
    "            # Step 3: Confidence Assessment\n",
    "            print(\"📊 Step 3: Confidence assessment...\")\n",
    "            confidence_assessment = self.modules['confidence'].process(query, initial_results)\n",
    "            pipeline.append('confidence_assessment')\n",
    "            \n",
    "            print(f\"   Confidence: {confidence_assessment.level.value} ({confidence_assessment.score:.2f})\")\n",
    "            print(f\"   Recommended action: {confidence_assessment.recommended_action.value}\")\n",
    "            print(f\"   Reasons: {', '.join(confidence_assessment.reasons[:2])}\")\n",
    "            \n",
    "            # Step 4: Corrective Action (if needed)\n",
    "            correction_result = None\n",
    "            final_documents = initial_results\n",
    "            correction_applied = False\n",
    "            \n",
    "            if confidence_assessment.recommended_action != CorrectionAction.NONE:\n",
    "                print(f\"🔧 Step 4: Applying correction - {confidence_assessment.recommended_action.value}...\")\n",
    "                \n",
    "                if confidence_assessment.recommended_action == CorrectionAction.WEB_SEARCH:\n",
    "                    correction_result = self.modules['web_search'].process(query)\n",
    "                elif confidence_assessment.recommended_action == CorrectionAction.DECOMPOSE_QUERY:\n",
    "                    correction_result = self.modules['decomposition'].process(query, self.modules['retrieval'])\n",
    "                elif confidence_assessment.recommended_action == CorrectionAction.REFINE_KNOWLEDGE:\n",
    "                    correction_result = self.modules['refinement'].process(query, self.modules['retrieval'])\n",
    "                else:  # HYBRID_SEARCH\n",
    "                    correction_result = self.modules['refinement'].process(query, self.modules['retrieval'])\n",
    "                \n",
    "                if correction_result and correction_result.success:\n",
    "                    final_documents = self._merge_results(initial_results, correction_result.new_documents)\n",
    "                    correction_applied = True\n",
    "                    self.corrections_applied += 1\n",
    "                    \n",
    "                    print(f\"   Correction successful: {len(correction_result.new_documents)} new documents\")\n",
    "                    print(f\"   Processing time: {correction_result.processing_time:.2f}s\")\n",
    "                else:\n",
    "                    print(f\"   Correction failed or not needed\")\n",
    "                \n",
    "                pipeline.append(f'correction_{confidence_assessment.recommended_action.value}')\n",
    "            else:\n",
    "                print(\"✅ Step 4: No correction needed - high confidence\")\n",
    "                pipeline.append('no_correction_needed')\n",
    "            \n",
    "            # Step 5: Answer Generation\n",
    "            print(\"🤖 Step 5: Travel advice generation...\")\n",
    "            generated_answer, final_confidence = self.modules['generation'].process(\n",
    "                query, final_documents, confidence_assessment, correction_applied\n",
    "            )\n",
    "            pipeline.append('generation')\n",
    "            \n",
    "            print(f\"   Generated response (Final confidence: {final_confidence:.2f})\")\n",
    "            \n",
    "            # Create response\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            response = CRAGResponse(\n",
    "                query=query,\n",
    "                initial_retrieval=initial_results,\n",
    "                confidence_assessment=confidence_assessment,\n",
    "                correction_result=correction_result,\n",
    "                final_documents=final_documents,\n",
    "                generated_answer=generated_answer,\n",
    "                overall_confidence=final_confidence,\n",
    "                processing_pipeline=pipeline,\n",
    "                processing_time=processing_time,\n",
    "                correction_applied=correction_applied\n",
    "            )\n",
    "            \n",
    "            # Update system metrics\n",
    "            self._update_metrics(response, confidence_assessment.score, final_confidence)\n",
    "            \n",
    "            print(f\"\\n✅ CRAG query processed in {processing_time:.2f}s\")\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            processing_time = time.time() - start_time\n",
    "            print(f\"❌ Error: {str(e)}\")\n",
    "            \n",
    "            # Return error response\n",
    "            error_query = TravelQuery(\"error\", query_text, TravelDomain.GENERAL, None, None, user_id)\n",
    "            error_assessment = ConfidenceAssessment(ConfidenceLevel.LOW, 0.0, [\"Error occurred\"], CorrectionAction.NONE)\n",
    "            \n",
    "            return CRAGResponse(\n",
    "                query=error_query,\n",
    "                initial_retrieval=[],\n",
    "                confidence_assessment=error_assessment,\n",
    "                correction_result=None,\n",
    "                final_documents=[],\n",
    "                generated_answer=f\"Sorry, I encountered an error processing your travel query: {str(e)}\",\n",
    "                overall_confidence=0.0,\n",
    "                processing_pipeline=['error'],\n",
    "                processing_time=processing_time,\n",
    "                correction_applied=False\n",
    "            )\n",
    "    \n",
    "    def _merge_results(self, initial_results: List[RetrievalResult], \n",
    "                      correction_results: List[RetrievalResult]) -> List[RetrievalResult]:\n",
    "        \"\"\"Merge initial and correction results, prioritizing corrected information\"\"\"\n",
    "        \n",
    "        # Start with correction results (higher priority due to freshness/accuracy)\n",
    "        merged_results = correction_results.copy()\n",
    "        \n",
    "        # Add initial results that don't conflict\n",
    "        correction_doc_ids = {r.document.id for r in correction_results}\n",
    "        \n",
    "        for result in initial_results:\n",
    "            if result.document.id not in correction_doc_ids:\n",
    "                # Slightly reduce score to show it's less fresh/verified\n",
    "                result.score *= 0.9\n",
    "                merged_results.append(result)\n",
    "        \n",
    "        # Sort by score and limit to top results\n",
    "        merged_results.sort(key=lambda x: x.score, reverse=True)\n",
    "        \n",
    "        # Re-assign ranks\n",
    "        for i, result in enumerate(merged_results[:7]):\n",
    "            result.rank = i + 1\n",
    "        \n",
    "        return merged_results[:7]\n",
    "    \n",
    "    def _update_metrics(self, response: CRAGResponse, initial_confidence: float, final_confidence: float):\n",
    "        \"\"\"Update system performance metrics\"\"\"\n",
    "        self.total_queries += 1\n",
    "        self.processing_times.append(response.processing_time)\n",
    "        \n",
    "        if response.correction_applied:\n",
    "            confidence_improvement = final_confidence - initial_confidence\n",
    "            self.avg_confidence_improvement = ((self.avg_confidence_improvement * (self.corrections_applied - 1)) + \n",
    "                                             confidence_improvement) / self.corrections_applied\n",
    "    \n",
    "    def get_system_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive system status and metrics\"\"\"\n",
    "        module_stats = {name: module.get_info() for name, module in self.modules.items()}\n",
    "        \n",
    "        return {\n",
    "            'total_queries': self.total_queries,\n",
    "            'corrections_applied': self.corrections_applied,\n",
    "            'correction_rate': (self.corrections_applied / max(self.total_queries, 1)) * 100,\n",
    "            'avg_confidence_improvement': self.avg_confidence_improvement,\n",
    "            'avg_processing_time': np.mean(self.processing_times) if self.processing_times else 0.0,\n",
    "            'module_stats': module_stats,\n",
    "            'correction_effectiveness': self.avg_confidence_improvement * 100\n",
    "        }\n",
    "\n",
    "# Initialize complete CRAG system\n",
    "corrective_rag = CorrectiveRAGSystem()\n",
    "print(\"\\n🚀 Complete Corrective RAG System ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b914da32-7b05-4e0d-96e7-b5cbf99657c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 CRAG Processing: 'Find luxury hotels in Bangkok for next week'\n",
      "============================================================\n",
      "📝 Step 1: Travel query analysis...\n",
      "   Domain: hotels, Location: Bangkok\n",
      "🔍 Step 2: Initial document retrieval...\n",
      "   Retrieved 5 documents\n",
      "   1. Grand Palace Hotel Bangkok - Luxury Accommodation (Score: 1.104, Freshness: 0.00)\n",
      "   2. Bangkok Street Food Guide - Must-Try Local Dishes (Score: 0.907, Freshness: 0.00)\n",
      "   3. Bangkok Public Transportation - BTS, MRT, and Taxis (Score: 0.864, Freshness: 0.00)\n",
      "📊 Step 3: Confidence assessment...\n",
      "   Confidence: low (0.33)\n",
      "   Recommended action: web_search\n",
      "   Reasons: Low confidence (score: 0.96), Information appears outdated\n",
      "🔧 Step 4: Applying correction - web_search...\n",
      "   Correction successful: 2 new documents\n",
      "   Processing time: 0.50s\n",
      "🤖 Step 5: Travel advice generation...\n",
      "   Generated response (Final confidence: 0.48)\n",
      "\n",
      "✅ CRAG query processed in 3.65s\n"
     ]
    }
   ],
   "source": [
    "# Process a travel query\n",
    "response = corrective_rag.process_query(\"Find luxury hotels in Bangkok for next week\")\n",
    "\n",
    "# Get system status\n",
    "status = corrective_rag.get_system_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513c001-3279-444d-afe9-0ed92bc5123e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e091a08d-ebff-4bf8-9470-0f6dd81912fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d047dbf-a254-43e5-ac49-67ec9a484993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
