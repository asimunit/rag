{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1427e5f8",
   "metadata": {},
   "source": [
    "# üß† Self-RAG Implementation\n",
    "\n",
    "## Self-Reflective Retrieval-Augmented Generation\n",
    "\n",
    "This notebook demonstrates a **Self-RAG system** with:\n",
    "- ü§î **Automatic Retrieval Decisions** - Model decides when to retrieve\n",
    "- üîç **Self-Reflection Tokens** - Special tokens for internal reasoning\n",
    "- üìä **Multi-Stage Evaluation** - Relevance, support, and utility assessment\n",
    "- üéØ **Adaptive Processing** - Dynamic response generation based on self-assessment\n",
    "- üß† **Meta-Cognitive Awareness** - Understanding of its own knowledge limitations\n",
    "\n",
    "### Key Innovation: Self-Reflection Tokens\n",
    "- **[Retrieve]**: Decision to retrieve additional information\n",
    "- **[ISREL]**: Relevance assessment of retrieved documents\n",
    "- **[ISSUP]**: Support evaluation - does retrieval support the answer?\n",
    "- **[ISUSE]**: Utility judgment - is the generated response useful?\n",
    "\n",
    "### Benefits of Self-RAG\n",
    "- **Intelligent Retrieval**: Only retrieves when necessary\n",
    "- **Quality Control**: Self-evaluates response quality\n",
    "- **Transparency**: Reasoning process is visible\n",
    "- **Adaptability**: Adjusts strategy based on confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ca0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install sentence-transformers faiss-cpu google-generativeai rank-bm25 transformers scikit-learn numpy python-dotenv torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82265c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass, field\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import google.generativeai as genai\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "print(\"üìö Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ef6ff",
   "metadata": {},
   "source": [
    "## üéØ Self-Reflection Token System\n",
    "\n",
    "Define the core self-reflection tokens and decision framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfafdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Reflection Tokens\n",
    "class ReflectionToken(Enum):\n",
    "    RETRIEVE = \"[Retrieve]\"\n",
    "    NO_RETRIEVE = \"[No Retrieve]\"\n",
    "    ISREL_YES = \"[ISREL: Yes]\"\n",
    "    ISREL_PARTIAL = \"[ISREL: Partial]\"\n",
    "    ISREL_NO = \"[ISREL: No]\"\n",
    "    ISSUP_FULL = \"[ISSUP: Fully Supported]\"\n",
    "    ISSUP_PARTIAL = \"[ISSUP: Partially Supported]\"\n",
    "    ISSUP_NO = \"[ISSUP: Not Supported]\"\n",
    "    ISUSE_HIGH = \"[ISUSE: High Utility]\"\n",
    "    ISUSE_MEDIUM = \"[ISUSE: Medium Utility]\"\n",
    "    ISUSE_LOW = \"[ISUSE: Low Utility]\"\n",
    "\n",
    "# Query complexity and confidence levels\n",
    "class ConfidenceLevel(Enum):\n",
    "    HIGH = \"high\"\n",
    "    MEDIUM = \"medium\"\n",
    "    LOW = \"low\"\n",
    "\n",
    "class QueryComplexity(Enum):\n",
    "    SIMPLE = \"simple\"\n",
    "    MODERATE = \"moderate\"\n",
    "    COMPLEX = \"complex\"\n",
    "\n",
    "class Domain(Enum):\n",
    "    TECHNOLOGY = \"technology\"\n",
    "    SCIENCE = \"science\"\n",
    "    MEDICINE = \"medicine\"\n",
    "    BUSINESS = \"business\"\n",
    "    GENERAL = \"general\"\n",
    "\n",
    "# Core data structures\n",
    "@dataclass\n",
    "class SelfRAGQuery:\n",
    "    id: str\n",
    "    text: str\n",
    "    domain: Domain\n",
    "    complexity: QueryComplexity\n",
    "    user_id: Optional[str] = None\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "@dataclass\n",
    "class RetrievalDecision:\n",
    "    should_retrieve: bool\n",
    "    confidence: float\n",
    "    reasoning: str\n",
    "    token: ReflectionToken\n",
    "\n",
    "@dataclass\n",
    "class RelevanceAssessment:\n",
    "    relevance_score: float\n",
    "    token: ReflectionToken\n",
    "    reasoning: str\n",
    "\n",
    "@dataclass\n",
    "class SupportEvaluation:\n",
    "    support_level: float\n",
    "    token: ReflectionToken\n",
    "    evidence: List[str]\n",
    "    reasoning: str\n",
    "\n",
    "@dataclass\n",
    "class UtilityJudgment:\n",
    "    utility_score: float\n",
    "    token: ReflectionToken\n",
    "    reasoning: str\n",
    "    improvement_suggestions: List[str]\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    id: str\n",
    "    title: str\n",
    "    content: str\n",
    "    domain: Domain\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "    keywords: List[str] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class SelfRAGResponse:\n",
    "    query: SelfRAGQuery\n",
    "    retrieval_decision: RetrievalDecision\n",
    "    retrieved_documents: List[Document]\n",
    "    relevance_assessment: Optional[RelevanceAssessment]\n",
    "    generated_answer: str\n",
    "    support_evaluation: SupportEvaluation\n",
    "    utility_judgment: UtilityJudgment\n",
    "    processing_time: float\n",
    "    reflection_chain: List[str]\n",
    "\n",
    "print(\"üéØ Self-Reflection token system defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3914cfa",
   "metadata": {},
   "source": [
    "## üìö Enhanced Knowledge Base\n",
    "\n",
    "Create a comprehensive knowledge base for testing Self-RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced knowledge base with medical and scientific content\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": \"tech_001\",\n",
    "        \"title\": \"Large Language Models and Transformers\",\n",
    "        \"domain\": \"technology\",\n",
    "        \"content\": \"Large Language Models (LLMs) like GPT, BERT, and T5 are based on the Transformer architecture introduced in 'Attention Is All You Need'. These models use self-attention mechanisms to process sequences and have revolutionized natural language processing. Key components include multi-head attention, position encodings, and feed-forward networks. Training involves massive datasets and requires significant computational resources.\",\n",
    "        \"keywords\": [\"LLM\", \"transformer\", \"attention\", \"BERT\", \"GPT\", \"NLP\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"med_001\",\n",
    "        \"title\": \"COVID-19 Symptoms and Treatment\",\n",
    "        \"domain\": \"medicine\",\n",
    "        \"content\": \"COVID-19 symptoms include fever, cough, shortness of breath, fatigue, muscle aches, headache, loss of taste or smell, sore throat, and congestion. Severe cases may develop pneumonia, acute respiratory distress syndrome (ARDS), or multi-organ failure. Treatment approaches include supportive care, antiviral medications like Paxlovid, monoclonal antibodies, and in severe cases, corticosteroids and mechanical ventilation. Vaccination remains the primary prevention strategy.\",\n",
    "        \"keywords\": [\"COVID-19\", \"symptoms\", \"treatment\", \"vaccine\", \"pneumonia\", \"antiviral\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"sci_001\",\n",
    "        \"title\": \"Quantum Entanglement and Bell's Theorem\",\n",
    "        \"domain\": \"science\",\n",
    "        \"content\": \"Quantum entanglement is a phenomenon where particles become correlated in such a way that the quantum state of each particle cannot be described independently. Bell's theorem demonstrates that no physical theory based on local hidden variables can reproduce all the predictions of quantum mechanics. Bell test experiments have consistently violated Bell inequalities, supporting quantum mechanics over local realism. This has implications for quantum computing, cryptography, and our understanding of reality.\",\n",
    "        \"keywords\": [\"quantum\", \"entanglement\", \"Bell theorem\", \"locality\", \"hidden variables\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"med_002\",\n",
    "        \"title\": \"Diabetes Management and Blood Sugar Control\",\n",
    "        \"domain\": \"medicine\",\n",
    "        \"content\": \"Diabetes management focuses on maintaining blood glucose levels within target ranges. Type 1 diabetes requires insulin therapy, while Type 2 may be managed with lifestyle modifications, oral medications (metformin, sulfonylureas), or insulin. Continuous glucose monitoring (CGM) and insulin pumps have improved management. Complications include diabetic retinopathy, nephropathy, neuropathy, and cardiovascular disease. Regular HbA1c testing monitors long-term glucose control.\",\n",
    "        \"keywords\": [\"diabetes\", \"insulin\", \"glucose\", \"HbA1c\", \"metformin\", \"CGM\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"tech_002\",\n",
    "        \"title\": \"Retrieval-Augmented Generation (RAG)\",\n",
    "        \"domain\": \"technology\",\n",
    "        \"content\": \"Retrieval-Augmented Generation (RAG) combines parametric knowledge from pre-trained language models with non-parametric knowledge from external sources. The system retrieves relevant documents from a knowledge base and uses them to augment the generation process. RAG variants include Self-RAG, which adds self-reflection capabilities, and GraphRAG, which uses graph structures. Benefits include factual accuracy, reduced hallucinations, and ability to incorporate up-to-date information.\",\n",
    "        \"keywords\": [\"RAG\", \"retrieval\", \"generation\", \"knowledge base\", \"hallucination\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"bus_001\",\n",
    "        \"title\": \"Artificial Intelligence in Healthcare Business\",\n",
    "        \"domain\": \"business\",\n",
    "        \"content\": \"AI in healthcare represents a $45 billion market with applications in diagnostics, drug discovery, personalized medicine, and operational efficiency. Key technologies include machine learning for medical imaging, natural language processing for clinical documentation, and predictive analytics for patient outcomes. Challenges include regulatory approval, data privacy (HIPAA compliance), integration with existing systems, and physician adoption. Success factors include clinical validation, workflow integration, and demonstrable ROI.\",\n",
    "        \"keywords\": [\"AI healthcare\", \"medical imaging\", \"drug discovery\", \"HIPAA\", \"clinical validation\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"sci_002\",\n",
    "        \"title\": \"CRISPR Gene Editing Technology\",\n",
    "        \"domain\": \"science\",\n",
    "        \"content\": \"CRISPR-Cas9 is a revolutionary gene editing tool that allows precise modification of DNA sequences. The system uses guide RNAs to direct the Cas9 nuclease to specific genomic locations where it creates double-strand breaks. This enables gene knockout, knock-in, or correction of mutations. Applications include treating genetic diseases, improving crops, and basic research. Ethical considerations include germline editing, off-target effects, and equitable access to therapies.\",\n",
    "        \"keywords\": [\"CRISPR\", \"gene editing\", \"Cas9\", \"guide RNA\", \"genetic disease\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"med_003\",\n",
    "        \"title\": \"Immunotherapy and Cancer Treatment\",\n",
    "        \"domain\": \"medicine\",\n",
    "        \"content\": \"Immunotherapy harnesses the immune system to fight cancer. Key approaches include checkpoint inhibitors (PD-1, PD-L1, CTLA-4 antibodies), CAR-T cell therapy, and cancer vaccines. Checkpoint inhibitors remove brakes on T-cells, allowing them to attack tumors. CAR-T involves modifying patient T-cells to recognize cancer antigens. Success rates vary by cancer type, with remarkable responses in melanoma, lung cancer, and certain blood cancers. Side effects include immune-related adverse events.\",\n",
    "        \"keywords\": [\"immunotherapy\", \"checkpoint inhibitors\", \"CAR-T\", \"PD-1\", \"cancer vaccine\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìö Enhanced knowledge base created with {len(knowledge_base)} documents\")\n",
    "print(f\"üåç Domains: {set(doc['domain'] for doc in knowledge_base)}\")\n",
    "print(f\"üìÑ Document types: Medical, Scientific, Technology, Business\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1babd9d",
   "metadata": {},
   "source": [
    "## üß† Self-Reflection Decision Engine\n",
    "\n",
    "Core component that makes intelligent retrieval decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfReflectionEngine:\n",
    "    def __init__(self):\n",
    "        self.confidence_threshold = 0.7\n",
    "        self.complexity_weights = {\n",
    "            QueryComplexity.SIMPLE: 0.3,\n",
    "            QueryComplexity.MODERATE: 0.6,\n",
    "            QueryComplexity.COMPLEX: 0.9\n",
    "        }\n",
    "        \n",
    "        # Domain-specific retrieval patterns\n",
    "        self.domain_retrieval_bias = {\n",
    "            Domain.MEDICINE: 0.8,  # High retrieval bias for medical queries\n",
    "            Domain.SCIENCE: 0.7,   # High for scientific queries\n",
    "            Domain.TECHNOLOGY: 0.6,\n",
    "            Domain.BUSINESS: 0.5,\n",
    "            Domain.GENERAL: 0.4\n",
    "        }\n",
    "        \n",
    "        print(\"üß† Self-Reflection Engine initialized\")\n",
    "    \n",
    "    def should_retrieve(self, query: SelfRAGQuery, initial_confidence: float) -> RetrievalDecision:\n",
    "        \"\"\"\n",
    "        Decide whether to retrieve additional information based on:\n",
    "        - Initial confidence in answering\n",
    "        - Query complexity\n",
    "        - Domain-specific requirements\n",
    "        - Potential risk of incorrect information\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate retrieval probability\n",
    "        complexity_factor = self.complexity_weights[query.complexity]\n",
    "        domain_factor = self.domain_retrieval_bias[query.domain]\n",
    "        confidence_factor = 1.0 - initial_confidence\n",
    "        \n",
    "        # Risk assessment for medical/scientific queries\n",
    "        risk_factor = 1.0\n",
    "        if query.domain in [Domain.MEDICINE, Domain.SCIENCE]:\n",
    "            risk_factor = 1.2  # Higher weight for high-stakes domains\n",
    "        \n",
    "        retrieval_score = (\n",
    "            complexity_factor * 0.3 +\n",
    "            domain_factor * 0.3 +\n",
    "            confidence_factor * 0.4\n",
    "        ) * risk_factor\n",
    "        \n",
    "        should_retrieve = retrieval_score > 0.5\n",
    "        \n",
    "        # Generate reasoning\n",
    "        if should_retrieve:\n",
    "            reasoning = self._generate_retrieval_reasoning(query, initial_confidence, retrieval_score)\n",
    "            token = ReflectionToken.RETRIEVE\n",
    "        else:\n",
    "            reasoning = f\"High confidence ({initial_confidence:.2f}) in existing knowledge for {query.complexity.value} {query.domain.value} query\"\n",
    "            token = ReflectionToken.NO_RETRIEVE\n",
    "        \n",
    "        return RetrievalDecision(\n",
    "            should_retrieve=should_retrieve,\n",
    "            confidence=retrieval_score,\n",
    "            reasoning=reasoning,\n",
    "            token=token\n",
    "        )\n",
    "    \n",
    "    def _generate_retrieval_reasoning(self, query: SelfRAGQuery, confidence: float, score: float) -> str:\n",
    "        reasons = []\n",
    "        \n",
    "        if confidence < 0.6:\n",
    "            reasons.append(f\"Low initial confidence ({confidence:.2f})\")\n",
    "        \n",
    "        if query.complexity in [QueryComplexity.MODERATE, QueryComplexity.COMPLEX]:\n",
    "            reasons.append(f\"Complex query requiring detailed information\")\n",
    "        \n",
    "        if query.domain in [Domain.MEDICINE, Domain.SCIENCE]:\n",
    "            reasons.append(f\"High-stakes {query.domain.value} domain requiring accuracy\")\n",
    "        \n",
    "        if \"latest\" in query.text.lower() or \"recent\" in query.text.lower():\n",
    "            reasons.append(\"Query requests current information\")\n",
    "        \n",
    "        return \"; \".join(reasons)\n",
    "    \n",
    "    def assess_relevance(self, query: SelfRAGQuery, documents: List[Document]) -> RelevanceAssessment:\n",
    "        \"\"\"\n",
    "        Assess how relevant retrieved documents are to the query\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return RelevanceAssessment(\n",
    "                relevance_score=0.0,\n",
    "                token=ReflectionToken.ISREL_NO,\n",
    "                reasoning=\"No documents retrieved\"\n",
    "            )\n",
    "        \n",
    "        # Simple relevance scoring based on keyword overlap and domain matching\n",
    "        relevance_scores = []\n",
    "        query_words = set(query.text.lower().split())\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc_words = set(doc.content.lower().split())\n",
    "            keyword_overlap = len(query_words.intersection(doc_words)) / len(query_words)\n",
    "            \n",
    "            domain_match = 1.0 if doc.domain == query.domain else 0.5\n",
    "            doc_score = keyword_overlap * 0.7 + domain_match * 0.3\n",
    "            relevance_scores.append(doc_score)\n",
    "        \n",
    "        avg_relevance = np.mean(relevance_scores)\n",
    "        \n",
    "        if avg_relevance >= 0.7:\n",
    "            token = ReflectionToken.ISREL_YES\n",
    "            reasoning = f\"High relevance ({avg_relevance:.2f}) - documents closely match query\"\n",
    "        elif avg_relevance >= 0.4:\n",
    "            token = ReflectionToken.ISREL_PARTIAL\n",
    "            reasoning = f\"Partial relevance ({avg_relevance:.2f}) - some useful information found\"\n",
    "        else:\n",
    "            token = ReflectionToken.ISREL_NO\n",
    "            reasoning = f\"Low relevance ({avg_relevance:.2f}) - documents don't match query well\"\n",
    "        \n",
    "        return RelevanceAssessment(\n",
    "            relevance_score=avg_relevance,\n",
    "            token=token,\n",
    "            reasoning=reasoning\n",
    "        )\n",
    "    \n",
    "    def evaluate_support(self, answer: str, documents: List[Document]) -> SupportEvaluation:\n",
    "        \"\"\"\n",
    "        Evaluate how well the retrieved documents support the generated answer\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return SupportEvaluation(\n",
    "                support_level=0.0,\n",
    "                token=ReflectionToken.ISSUP_NO,\n",
    "                evidence=[],\n",
    "                reasoning=\"No retrieved documents to provide support\"\n",
    "            )\n",
    "        \n",
    "        # Extract key facts from answer\n",
    "        answer_sentences = [s.strip() for s in answer.split('.') if s.strip()]\n",
    "        evidence = []\n",
    "        support_scores = []\n",
    "        \n",
    "        for sentence in answer_sentences[:3]:  # Check first 3 sentences\n",
    "            sentence_words = set(sentence.lower().split())\n",
    "            max_support = 0.0\n",
    "            supporting_doc = None\n",
    "            \n",
    "            for doc in documents:\n",
    "                doc_words = set(doc.content.lower().split())\n",
    "                overlap = len(sentence_words.intersection(doc_words)) / max(len(sentence_words), 1)\n",
    "                if overlap > max_support:\n",
    "                    max_support = overlap\n",
    "                    supporting_doc = doc\n",
    "            \n",
    "            if max_support > 0.3 and supporting_doc:\n",
    "                evidence.append(f\"'{sentence}' supported by {supporting_doc.title}\")\n",
    "                support_scores.append(max_support)\n",
    "        \n",
    "        avg_support = np.mean(support_scores) if support_scores else 0.0\n",
    "        \n",
    "        if avg_support >= 0.7:\n",
    "            token = ReflectionToken.ISSUP_FULL\n",
    "            reasoning = f\"Answer is fully supported by retrieved documents ({avg_support:.2f})\"\n",
    "        elif avg_support >= 0.4:\n",
    "            token = ReflectionToken.ISSUP_PARTIAL\n",
    "            reasoning = f\"Answer is partially supported by retrieved documents ({avg_support:.2f})\"\n",
    "        else:\n",
    "            token = ReflectionToken.ISSUP_NO\n",
    "            reasoning = f\"Answer lacks strong support from retrieved documents ({avg_support:.2f})\"\n",
    "        \n",
    "        return SupportEvaluation(\n",
    "            support_level=avg_support,\n",
    "            token=token,\n",
    "            evidence=evidence,\n",
    "            reasoning=reasoning\n",
    "        )\n",
    "    \n",
    "    def judge_utility(self, query: SelfRAGQuery, answer: str, support_eval: SupportEvaluation) -> UtilityJudgment:\n",
    "        \"\"\"\n",
    "        Judge the overall utility and quality of the generated response\n",
    "        \"\"\"\n",
    "        utility_factors = []\n",
    "        \n",
    "        # Answer length and completeness\n",
    "        length_score = min(len(answer.split()) / 50, 1.0)  # Normalize to 50 words\n",
    "        utility_factors.append(length_score * 0.2)\n",
    "        \n",
    "        # Support from retrieved documents\n",
    "        utility_factors.append(support_eval.support_level * 0.4)\n",
    "        \n",
    "        # Domain appropriateness\n",
    "        domain_keywords = {\n",
    "            Domain.MEDICINE: ['treatment', 'symptoms', 'diagnosis', 'therapy', 'medical'],\n",
    "            Domain.SCIENCE: ['research', 'study', 'experiment', 'theory', 'scientific'],\n",
    "            Domain.TECHNOLOGY: ['system', 'algorithm', 'software', 'technology', 'technical'],\n",
    "            Domain.BUSINESS: ['market', 'business', 'revenue', 'strategy', 'commercial']\n",
    "        }\n",
    "        \n",
    "        if query.domain in domain_keywords:\n",
    "            domain_match = sum(1 for word in domain_keywords[query.domain] \n",
    "                             if word in answer.lower()) / len(domain_keywords[query.domain])\n",
    "            utility_factors.append(domain_match * 0.2)\n",
    "        else:\n",
    "            utility_factors.append(0.1)  # Default for general domain\n",
    "        \n",
    "        # Specificity and detail\n",
    "        specific_indicators = ['specifically', 'exactly', 'precisely', 'including', 'such as']\n",
    "        specificity_score = sum(1 for indicator in specific_indicators \n",
    "                              if indicator in answer.lower()) / len(specific_indicators)\n",
    "        utility_factors.append(specificity_score * 0.2)\n",
    "        \n",
    "        utility_score = sum(utility_factors)\n",
    "        \n",
    "        # Generate improvement suggestions\n",
    "        suggestions = []\n",
    "        if length_score < 0.5:\n",
    "            suggestions.append(\"Provide more detailed explanation\")\n",
    "        if support_eval.support_level < 0.5:\n",
    "            suggestions.append(\"Better utilize retrieved information\")\n",
    "        if specificity_score < 0.3:\n",
    "            suggestions.append(\"Include more specific examples and details\")\n",
    "        \n",
    "        # Determine utility level\n",
    "        if utility_score >= 0.7:\n",
    "            token = ReflectionToken.ISUSE_HIGH\n",
    "            reasoning = f\"High utility response ({utility_score:.2f}) - comprehensive and well-supported\"\n",
    "        elif utility_score >= 0.4:\n",
    "            token = ReflectionToken.ISUSE_MEDIUM\n",
    "            reasoning = f\"Medium utility response ({utility_score:.2f}) - adequate but could be improved\"\n",
    "        else:\n",
    "            token = ReflectionToken.ISUSE_LOW\n",
    "            reasoning = f\"Low utility response ({utility_score:.2f}) - needs significant improvement\"\n",
    "        \n",
    "        return UtilityJudgment(\n",
    "            utility_score=utility_score,\n",
    "            token=token,\n",
    "            reasoning=reasoning,\n",
    "            improvement_suggestions=suggestions\n",
    "        )\n",
    "\n",
    "# Initialize self-reflection engine\n",
    "reflection_engine = SelfReflectionEngine()\n",
    "print(\"‚úÖ Self-Reflection Engine ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25af12",
   "metadata": {},
   "source": [
    "## üîç Enhanced Retrieval System\n",
    "\n",
    "Retrieval system optimized for Self-RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40192b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfRAGRetriever:\n",
    "    def __init__(self):\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.documents = []\n",
    "        self.semantic_index = None\n",
    "        self.bm25_index = None\n",
    "        \n",
    "        print(\"üîç Self-RAG Retriever initialized\")\n",
    "    \n",
    "    def index_documents(self, documents: List[Dict]):\n",
    "        print(f\"üìö Indexing {len(documents)} documents for Self-RAG...\")\n",
    "        \n",
    "        # Convert to Document objects\n",
    "        self.documents = [\n",
    "            Document(\n",
    "                id=doc['id'],\n",
    "                title=doc['title'],\n",
    "                content=doc['content'],\n",
    "                domain=Domain(doc['domain']),\n",
    "                keywords=doc.get('keywords', [])\n",
    "            )\n",
    "            for doc in documents\n",
    "        ]\n",
    "        \n",
    "        # Build semantic index\n",
    "        self._build_semantic_index()\n",
    "        \n",
    "        # Build keyword index\n",
    "        self._build_keyword_index()\n",
    "        \n",
    "        print(\"‚úÖ Documents indexed successfully!\")\n",
    "    \n",
    "    def _build_semantic_index(self):\n",
    "        doc_texts = [f\"{doc.title} {doc.content}\" for doc in self.documents]\n",
    "        embeddings = self.embedding_model.encode(doc_texts)\n",
    "        \n",
    "        # Store embeddings\n",
    "        for doc, embedding in zip(self.documents, embeddings):\n",
    "            doc.embedding = embedding\n",
    "        \n",
    "        # Create FAISS index\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.semantic_index = faiss.IndexFlatIP(dimension)\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.semantic_index.add(embeddings.astype('float32'))\n",
    "    \n",
    "    def _build_keyword_index(self):\n",
    "        doc_texts = [f\"{doc.title} {doc.content}\" for doc in self.documents]\n",
    "        tokenized_docs = [text.lower().split() for text in doc_texts]\n",
    "        self.bm25_index = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    def retrieve(self, query: SelfRAGQuery, top_k: int = 5) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents using hybrid approach\n",
    "        \"\"\"\n",
    "        # Semantic retrieval\n",
    "        query_embedding = self.embedding_model.encode([query.text])\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        semantic_scores, semantic_indices = self.semantic_index.search(\n",
    "            query_embedding.astype('float32'), min(top_k * 2, len(self.documents))\n",
    "        )\n",
    "        \n",
    "        # Keyword retrieval\n",
    "        query_tokens = query.text.lower().split()\n",
    "        keyword_scores = self.bm25_index.get_scores(query_tokens)\n",
    "        keyword_indices = np.argsort(keyword_scores)[::-1][:top_k * 2]\n",
    "        \n",
    "        # Combine and rank\n",
    "        combined_scores = {}\n",
    "        \n",
    "        # Add semantic scores\n",
    "        for score, idx in zip(semantic_scores[0], semantic_indices[0]):\n",
    "            if idx < len(self.documents):  # Valid index check\n",
    "                combined_scores[idx] = {'semantic': float(score), 'keyword': 0.0}\n",
    "        \n",
    "        # Add keyword scores (normalized)\n",
    "        max_keyword_score = max(keyword_scores) if max(keyword_scores) > 0 else 1.0\n",
    "        for idx in keyword_indices:\n",
    "            if idx < len(self.documents):  # Valid index check\n",
    "                normalized_score = keyword_scores[idx] / max_keyword_score\n",
    "                if idx in combined_scores:\n",
    "                    combined_scores[idx]['keyword'] = normalized_score\n",
    "                else:\n",
    "                    combined_scores[idx] = {'semantic': 0.0, 'keyword': normalized_score}\n",
    "        \n",
    "        # Calculate final scores with domain boost\n",
    "        final_scores = []\n",
    "        for idx, scores in combined_scores.items():\n",
    "            doc = self.documents[idx]\n",
    "            \n",
    "            # Domain matching boost\n",
    "            domain_boost = 1.2 if doc.domain == query.domain else 1.0\n",
    "            \n",
    "            final_score = (scores['semantic'] * 0.7 + scores['keyword'] * 0.3) * domain_boost\n",
    "            final_scores.append((final_score, idx))\n",
    "        \n",
    "        # Sort and return top documents\n",
    "        final_scores.sort(reverse=True)\n",
    "        retrieved_docs = [self.documents[idx] for _, idx in final_scores[:top_k]]\n",
    "        \n",
    "        return retrieved_docs\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = SelfRAGRetriever()\n",
    "retriever.index_documents(knowledge_base)\n",
    "print(\"‚úÖ Self-RAG Retriever ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63087dce",
   "metadata": {},
   "source": [
    "## ü§ñ Self-Aware Generator\n",
    "\n",
    "Generation module with self-awareness capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2851306",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAwareGenerator:\n",
    "    def __init__(self):\n",
    "        # Try to initialize Gemini\n",
    "        api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if api_key:\n",
    "            try:\n",
    "                genai.configure(api_key=api_key)\n",
    "                self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "                self.has_llm = True\n",
    "                print(\"ü§ñ Gemini API configured for Self-RAG\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Gemini error: {e}. Using fallback generation.\")\n",
    "                self.has_llm = False\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No Gemini API key. Using template-based generation.\")\n",
    "            self.has_llm = False\n",
    "    \n",
    "    def estimate_confidence(self, query: SelfRAGQuery) -> float:\n",
    "        \"\"\"\n",
    "        Estimate initial confidence for answering query without retrieval\n",
    "        \"\"\"\n",
    "        # Base confidence on complexity and domain\n",
    "        complexity_confidence = {\n",
    "            QueryComplexity.SIMPLE: 0.8,\n",
    "            QueryComplexity.MODERATE: 0.6,\n",
    "            QueryComplexity.COMPLEX: 0.4\n",
    "        }\n",
    "        \n",
    "        domain_confidence = {\n",
    "            Domain.GENERAL: 0.7,\n",
    "            Domain.TECHNOLOGY: 0.6,\n",
    "            Domain.BUSINESS: 0.6,\n",
    "            Domain.SCIENCE: 0.4,\n",
    "            Domain.MEDICINE: 0.3  # Lower confidence for medical queries\n",
    "        }\n",
    "        \n",
    "        base_confidence = complexity_confidence[query.complexity] * domain_confidence[query.domain]\n",
    "        \n",
    "        # Adjust for specific query patterns\n",
    "        uncertainty_indicators = ['latest', 'recent', 'current', 'new', 'specific', 'exact']\n",
    "        if any(indicator in query.text.lower() for indicator in uncertainty_indicators):\n",
    "            base_confidence *= 0.7\n",
    "        \n",
    "        return min(base_confidence, 0.95)  # Cap at 95%\n",
    "    \n",
    "    def generate_answer(self, query: SelfRAGQuery, documents: List[Document] = None, \n",
    "                       retrieval_decision: RetrievalDecision = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer with self-awareness of limitations\n",
    "        \"\"\"\n",
    "        if self.has_llm:\n",
    "            return self._generate_with_llm(query, documents, retrieval_decision)\n",
    "        else:\n",
    "            return self._generate_with_template(query, documents, retrieval_decision)\n",
    "    \n",
    "    def _generate_with_llm(self, query: SelfRAGQuery, documents: List[Document], \n",
    "                          retrieval_decision: RetrievalDecision) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer using Gemini with self-reflection context\n",
    "        \"\"\"\n",
    "        # Prepare context\n",
    "        context_parts = []\n",
    "        if documents:\n",
    "            for doc in documents:\n",
    "                context_parts.append(f\"**{doc.title}**\\n{doc.content}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts) if context_parts else \"No additional context provided.\"\n",
    "        \n",
    "        # Create self-aware prompt\n",
    "        prompt = f\"\"\"You are an AI assistant with self-reflection capabilities. \n",
    "\n",
    "**Query Analysis:**\n",
    "- Question: {query.text}\n",
    "- Domain: {query.domain.value}\n",
    "- Complexity: {query.complexity.value}\n",
    "- Retrieval Decision: {retrieval_decision.token.value if retrieval_decision else 'Not specified'}\n",
    "- Reasoning: {retrieval_decision.reasoning if retrieval_decision else 'Not provided'}\n",
    "\n",
    "**Retrieved Context:**\n",
    "{context}\n",
    "\n",
    "**Instructions:**\n",
    "1. Provide a comprehensive answer based on the available information\n",
    "2. If using retrieved context, clearly integrate the information\n",
    "3. Be explicit about any limitations or uncertainties\n",
    "4. For medical/scientific queries, emphasize the need for professional consultation\n",
    "5. Maintain appropriate confidence level based on available evidence\n",
    "\n",
    "**Answer:**\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def _generate_with_template(self, query: SelfRAGQuery, documents: List[Document], \n",
    "                               retrieval_decision: RetrievalDecision) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer using template-based approach\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            answer = f\"Based on my general knowledge about {query.domain.value}, \"\n",
    "            \n",
    "            if query.domain == Domain.MEDICINE:\n",
    "                answer += \"I can provide general information, but please consult healthcare professionals for medical advice. \"\n",
    "            elif query.domain == Domain.SCIENCE:\n",
    "                answer += \"I can share scientific concepts, but specific research may require current literature. \"\n",
    "            \n",
    "            answer += f\"Regarding '{query.text}': This is a {query.complexity.value} question that \"\n",
    "            \n",
    "            if retrieval_decision and not retrieval_decision.should_retrieve:\n",
    "                answer += \"I feel confident addressing with existing knowledge.\"\n",
    "            else:\n",
    "                answer += \"would benefit from additional specialized resources.\"\n",
    "            \n",
    "            return answer\n",
    "        \n",
    "        # Use retrieved documents\n",
    "        answer = f\"Based on the retrieved information about {query.domain.value}, here's what I found regarding '{query.text}':\\n\\n\"\n",
    "        \n",
    "        # Incorporate information from top documents\n",
    "        for i, doc in enumerate(documents[:2], 1):\n",
    "            answer += f\"**{doc.title}**: {doc.content[:300]}..\\n\\n\"\n",
    "        \n",
    "        # Add appropriate disclaimers\n",
    "        if query.domain == Domain.MEDICINE:\n",
    "            answer += \"\\n**Important**: This information is for educational purposes only. Please consult qualified healthcare professionals for medical advice.\"\n",
    "        elif query.domain == Domain.SCIENCE:\n",
    "            answer += \"\\n**Note**: Scientific understanding evolves. For the latest research, consult current peer-reviewed literature.\"\n",
    "        \n",
    "        return answer\n",
    "\n",
    "# Initialize generator\n",
    "generator = SelfAwareGenerator()\n",
    "print(\"‚úÖ Self-Aware Generator ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d6790b",
   "metadata": {},
   "source": [
    "## üìù Query Processing and Classification\n",
    "\n",
    "Intelligent query understanding for Self-RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d364b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfRAGQueryProcessor:\n",
    "    def __init__(self):\n",
    "        # Complexity indicators\n",
    "        self.complexity_patterns = {\n",
    "            QueryComplexity.SIMPLE: [\n",
    "                r\"what is\", r\"define\", r\"explain\", r\"tell me about\"\n",
    "            ],\n",
    "            QueryComplexity.MODERATE: [\n",
    "                r\"how does\", r\"why\", r\"compare\", r\"difference between\", \n",
    "                r\"advantages\", r\"disadvantages\"\n",
    "            ],\n",
    "            QueryComplexity.COMPLEX: [\n",
    "                r\"analyze\", r\"evaluate\", r\"assess\", r\"relationship between\",\n",
    "                r\"impact of\", r\"implications\", r\"comprehensive\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Domain keywords\n",
    "        self.domain_keywords = {\n",
    "            Domain.MEDICINE: [\n",
    "                \"disease\", \"treatment\", \"symptoms\", \"diagnosis\", \"therapy\", \n",
    "                \"medication\", \"clinical\", \"patient\", \"medical\", \"health\",\n",
    "                \"cancer\", \"diabetes\", \"COVID\", \"vaccine\", \"drug\"\n",
    "            ],\n",
    "            Domain.SCIENCE: [\n",
    "                \"research\", \"study\", \"experiment\", \"theory\", \"hypothesis\",\n",
    "                \"quantum\", \"physics\", \"chemistry\", \"biology\", \"genetics\",\n",
    "                \"CRISPR\", \"evolution\", \"molecular\", \"scientific\"\n",
    "            ],\n",
    "            Domain.TECHNOLOGY: [\n",
    "                \"AI\", \"algorithm\", \"software\", \"computing\", \"system\",\n",
    "                \"machine learning\", \"neural network\", \"programming\",\n",
    "                \"database\", \"cloud\", \"blockchain\", \"cybersecurity\"\n",
    "            ],\n",
    "            Domain.BUSINESS: [\n",
    "                \"market\", \"revenue\", \"profit\", \"strategy\", \"business\",\n",
    "                \"investment\", \"economy\", \"finance\", \"management\",\n",
    "                \"startup\", \"company\", \"industry\", \"commercial\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        print(\"üìù Self-RAG Query Processor initialized\")\n",
    "    \n",
    "    def process_query(self, query_text: str, user_id: str = None) -> SelfRAGQuery:\n",
    "        \"\"\"\n",
    "        Process and classify query for Self-RAG\n",
    "        \"\"\"\n",
    "        query_id = str(uuid.uuid4())[:8]\n",
    "        \n",
    "        # Detect complexity\n",
    "        complexity = self._detect_complexity(query_text)\n",
    "        \n",
    "        # Detect domain\n",
    "        domain = self._detect_domain(query_text)\n",
    "        \n",
    "        return SelfRAGQuery(\n",
    "            id=query_id,\n",
    "            text=query_text,\n",
    "            domain=domain,\n",
    "            complexity=complexity,\n",
    "            user_id=user_id\n",
    "        )\n",
    "    \n",
    "    def _detect_complexity(self, text: str) -> QueryComplexity:\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check for complex patterns first\n",
    "        for complexity, patterns in reversed(list(self.complexity_patterns.items())):\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, text_lower):\n",
    "                    return complexity\n",
    "        \n",
    "        # Fallback based on length and structure\n",
    "        word_count = len(text.split())\n",
    "        if word_count > 15:\n",
    "            return QueryComplexity.COMPLEX\n",
    "        elif word_count > 8:\n",
    "            return QueryComplexity.MODERATE\n",
    "        else:\n",
    "            return QueryComplexity.SIMPLE\n",
    "    \n",
    "    def _detect_domain(self, text: str) -> Domain:\n",
    "        text_lower = text.lower()\n",
    "        domain_scores = {}\n",
    "        \n",
    "        for domain, keywords in self.domain_keywords.items():\n",
    "            score = sum(1 for keyword in keywords if keyword.lower() in text_lower)\n",
    "            domain_scores[domain] = score\n",
    "        \n",
    "        if max(domain_scores.values()) > 0:\n",
    "            return max(domain_scores, key=domain_scores.get)\n",
    "        return Domain.GENERAL\n",
    "\n",
    "# Initialize query processor\n",
    "query_processor = SelfRAGQueryProcessor()\n",
    "\n",
    "# Test the processor\n",
    "test_query = query_processor.process_query(\"What are the latest treatments for diabetes?\")\n",
    "print(f\"\\nüîç Test query processed:\")\n",
    "print(f\"   Text: {test_query.text}\")\n",
    "print(f\"   Domain: {test_query.domain.value}\")\n",
    "print(f\"   Complexity: {test_query.complexity.value}\")\n",
    "print(\"‚úÖ Query Processor ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc913bfc",
   "metadata": {},
   "source": [
    "## üß© Complete Self-RAG System\n",
    "\n",
    "Integration of all components into the complete Self-RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aff874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfRAGSystem:\n",
    "    def __init__(self):\n",
    "        print(\"üß© Initializing Self-RAG System...\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.query_processor = query_processor\n",
    "        self.reflection_engine = reflection_engine\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "        \n",
    "        # System metrics\n",
    "        self.total_queries = 0\n",
    "        self.retrieval_decisions = {'retrieve': 0, 'no_retrieve': 0}\n",
    "        self.avg_processing_time = 0.0\n",
    "        self.quality_scores = []\n",
    "        \n",
    "        print(\"‚úÖ Self-RAG System initialized!\")\n",
    "        print(\"üéØ Ready for self-reflective question answering\")\n",
    "    \n",
    "    def process_query(self, query_text: str, user_id: str = None) -> SelfRAGResponse:\n",
    "        \"\"\"\n",
    "        Process query through complete Self-RAG pipeline\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        reflection_chain = []\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nüß† Self-RAG Processing: '{query_text}'\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            # Step 1: Query Processing\n",
    "            print(\"üìù Step 1: Query analysis and classification...\")\n",
    "            query = self.query_processor.process_query(query_text, user_id)\n",
    "            reflection_chain.append(f\"Query classified as {query.complexity.value} {query.domain.value} question\")\n",
    "            \n",
    "            print(f\"   Domain: {query.domain.value}\")\n",
    "            print(f\"   Complexity: {query.complexity.value}\")\n",
    "            \n",
    "            # Step 2: Initial Confidence Assessment\n",
    "            print(\"üéØ Step 2: Self-confidence assessment...\")\n",
    "            initial_confidence = self.generator.estimate_confidence(query)\n",
    "            reflection_chain.append(f\"Initial confidence: {initial_confidence:.2f}\")\n",
    "            \n",
    "            print(f\"   Initial confidence: {initial_confidence:.2f}\")\n",
    "            \n",
    "            # Step 3: Retrieval Decision (Self-Reflection)\n",
    "            print(\"ü§î Step 3: Retrieval decision with self-reflection...\")\n",
    "            retrieval_decision = self.reflection_engine.should_retrieve(query, initial_confidence)\n",
    "            reflection_chain.append(f\"{retrieval_decision.token.value}: {retrieval_decision.reasoning}\")\n",
    "            \n",
    "            print(f\"   Decision: {retrieval_decision.token.value}\")\n",
    "            print(f\"   Reasoning: {retrieval_decision.reasoning}\")\n",
    "            \n",
    "            # Update decision metrics\n",
    "            if retrieval_decision.should_retrieve:\n",
    "                self.retrieval_decisions['retrieve'] += 1\n",
    "            else:\n",
    "                self.retrieval_decisions['no_retrieve'] += 1\n",
    "            \n",
    "            # Step 4: Conditional Retrieval\n",
    "            retrieved_documents = []\n",
    "            relevance_assessment = None\n",
    "            \n",
    "            if retrieval_decision.should_retrieve:\n",
    "                print(\"üîç Step 4: Document retrieval...\")\n",
    "                retrieved_documents = self.retriever.retrieve(query, top_k=5)\n",
    "                reflection_chain.append(f\"Retrieved {len(retrieved_documents)} documents\")\n",
    "                \n",
    "                print(f\"   Retrieved {len(retrieved_documents)} documents\")\n",
    "                for i, doc in enumerate(retrieved_documents[:3], 1):\n",
    "                    print(f\"   {i}. {doc.title} ({doc.domain.value})\")\n",
    "                \n",
    "                # Step 5: Relevance Assessment\n",
    "                print(\"üìä Step 5: Relevance assessment...\")\n",
    "                relevance_assessment = self.reflection_engine.assess_relevance(query, retrieved_documents)\n",
    "                reflection_chain.append(f\"{relevance_assessment.token.value}: {relevance_assessment.reasoning}\")\n",
    "                \n",
    "                print(f\"   Assessment: {relevance_assessment.token.value}\")\n",
    "                print(f\"   Score: {relevance_assessment.relevance_score:.2f}\")\n",
    "            else:\n",
    "                print(\"üö´ Step 4: Skipping retrieval based on self-assessment\")\n",
    "                reflection_chain.append(\"Retrieval skipped - high confidence in existing knowledge\")\n",
    "            \n",
    "            # Step 6: Answer Generation\n",
    "            print(\"ü§ñ Step 6: Self-aware answer generation...\")\n",
    "            generated_answer = self.generator.generate_answer(query, retrieved_documents, retrieval_decision)\n",
    "            reflection_chain.append(f\"Generated answer ({len(generated_answer.split())} words)\")\n",
    "            \n",
    "            print(f\"   Generated answer ({len(generated_answer.split())} words)\")\n",
    "            \n",
    "            # Step 7: Support Evaluation\n",
    "            print(\"üî¨ Step 7: Support evaluation...\")\n",
    "            support_evaluation = self.reflection_engine.evaluate_support(generated_answer, retrieved_documents)\n",
    "            reflection_chain.append(f\"{support_evaluation.token.value}: {support_evaluation.reasoning}\")\n",
    "            \n",
    "            print(f\"   Support: {support_evaluation.token.value}\")\n",
    "            print(f\"   Evidence count: {len(support_evaluation.evidence)}\")\n",
    "            \n",
    "            # Step 8: Utility Judgment\n",
    "            print(\"‚öñÔ∏è Step 8: Utility judgment...\")\n",
    "            utility_judgment = self.reflection_engine.judge_utility(query, generated_answer, support_evaluation)\n",
    "            reflection_chain.append(f\"{utility_judgment.token.value}: {utility_judgment.reasoning}\")\n",
    "            \n",
    "            print(f\"   Utility: {utility_judgment.token.value}\")\n",
    "            print(f\"   Score: {utility_judgment.utility_score:.2f}\")\n",
    "            \n",
    "            # Create response\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            response = SelfRAGResponse(\n",
    "                query=query,\n",
    "                retrieval_decision=retrieval_decision,\n",
    "                retrieved_documents=retrieved_documents,\n",
    "                relevance_assessment=relevance_assessment,\n",
    "                generated_answer=generated_answer,\n",
    "                support_evaluation=support_evaluation,\n",
    "                utility_judgment=utility_judgment,\n",
    "                processing_time=processing_time,\n",
    "                reflection_chain=reflection_chain\n",
    "            )\n",
    "            \n",
    "            # Update system metrics\n",
    "            self._update_metrics(response)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Self-RAG processing completed in {processing_time:.2f}s\")\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in Self-RAG processing: {str(e)}\")\n",
    "            # Return error response\n",
    "            error_query = SelfRAGQuery(\"error\", query_text, Domain.GENERAL, QueryComplexity.SIMPLE, user_id)\n",
    "            error_decision = RetrievalDecision(False, 0.0, \"Error occurred\", ReflectionToken.NO_RETRIEVE)\n",
    "            error_support = SupportEvaluation(0.0, ReflectionToken.ISSUP_NO, [], \"Error occurred\")\n",
    "            error_utility = UtilityJudgment(0.0, ReflectionToken.ISUSE_LOW, \"Error occurred\", [])\n",
    "            \n",
    "            return SelfRAGResponse(\n",
    "                query=error_query,\n",
    "                retrieval_decision=error_decision,\n",
    "                retrieved_documents=[],\n",
    "                relevance_assessment=None,\n",
    "                generated_answer=f\"I encountered an error while processing your query: {str(e)}\",\n",
    "                support_evaluation=error_support,\n",
    "                utility_judgment=error_utility,\n",
    "                processing_time=time.time() - start_time,\n",
    "                reflection_chain=[f\"Error: {str(e)}\"]\n",
    "            )\n",
    "    \n",
    "    def _update_metrics(self, response: SelfRAGResponse):\n",
    "        self.total_queries += 1\n",
    "        \n",
    "        # Update running average processing time\n",
    "        self.avg_processing_time = ((self.avg_processing_time * (self.total_queries - 1)) + \n",
    "                                   response.processing_time) / self.total_queries\n",
    "        \n",
    "        # Track quality scores\n",
    "        self.quality_scores.append(response.utility_judgment.utility_score)\n",
    "    \n",
    "    def get_system_statistics(self) -> Dict:\n",
    "        return {\n",
    "            'total_queries': self.total_queries,\n",
    "            'retrieval_rate': self.retrieval_decisions['retrieve'] / max(self.total_queries, 1),\n",
    "            'avg_processing_time': self.avg_processing_time,\n",
    "            'avg_quality_score': np.mean(self.quality_scores) if self.quality_scores else 0.0,\n",
    "            'retrieval_decisions': self.retrieval_decisions\n",
    "        }\n",
    "\n",
    "# Initialize complete Self-RAG system\n",
    "self_rag = SelfRAGSystem()\n",
    "print(\"\\nüöÄ Complete Self-RAG System ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52efc5d",
   "metadata": {},
   "source": [
    "## üß™ Comprehensive Self-RAG Testing\n",
    "\n",
    "Test the Self-RAG system with various query types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self_rag_tests():\n",
    "    print(\"\\n\" + \"üß™\" * 20 + \" SELF-RAG TEST SUITE \" + \"üß™\" * 20)\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"Simple Medical Query\",\n",
    "            \"query\": \"What are the symptoms of diabetes?\",\n",
    "            \"expected_retrieval\": True,  # Medical queries should trigger retrieval\n",
    "            \"user_id\": \"medical_user_1\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Complex Scientific Analysis\",\n",
    "            \"query\": \"Analyze the implications of quantum entanglement for computing applications\",\n",
    "            \"expected_retrieval\": True,\n",
    "            \"user_id\": \"scientist_1\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Simple Technology Question\",\n",
    "            \"query\": \"What is machine learning?\",\n",
    "            \"expected_retrieval\": False,  # Might not need retrieval\n",
    "            \"user_id\": \"tech_user_1\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Current Medical Treatment\",\n",
    "            \"query\": \"What are the latest treatments for COVID-19?\",\n",
    "            \"expected_retrieval\": True,  # \"Latest\" should trigger retrieval\n",
    "            \"user_id\": \"medical_user_2\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Business Strategy Query\",\n",
    "            \"query\": \"How is AI transforming healthcare business models?\",\n",
    "            \"expected_retrieval\": True,\n",
    "            \"user_id\": \"business_user_1\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"üî¨ TEST CASE {i}: {test_case['name']}\")\n",
    "        print(f\"‚ùì Query: '{test_case['query']}'\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        try:\n",
    "            response = self_rag.process_query(\n",
    "                test_case['query'], \n",
    "                test_case['user_id']\n",
    "            )\n",
    "            \n",
    "            # Analyze results\n",
    "            print(f\"\\nüéØ **FINAL ANSWER:**\")\n",
    "            print(f\"   {response.generated_answer[:300]}{'...' if len(response.generated_answer) > 300 else ''}\")\n",
    "            \n",
    "            print(f\"\\nüß† **SELF-REFLECTION ANALYSIS:**\")\n",
    "            print(f\"   ‚Ä¢ Retrieval Decision: {response.retrieval_decision.token.value}\")\n",
    "            print(f\"   ‚Ä¢ Retrieved Documents: {len(response.retrieved_documents)}\")\n",
    "            if response.relevance_assessment:\n",
    "                print(f\"   ‚Ä¢ Relevance: {response.relevance_assessment.token.value}\")\n",
    "            print(f\"   ‚Ä¢ Support: {response.support_evaluation.token.value}\")\n",
    "            print(f\"   ‚Ä¢ Utility: {response.utility_judgment.token.value}\")\n",
    "            \n",
    "            print(f\"\\nüìä **METRICS:**\")\n",
    "            print(f\"   ‚Ä¢ Processing Time: {response.processing_time:.2f}s\")\n",
    "            print(f\"   ‚Ä¢ Quality Score: {response.utility_judgment.utility_score:.2f}\")\n",
    "            print(f\"   ‚Ä¢ Support Score: {response.support_evaluation.support_level:.2f}\")\n",
    "            \n",
    "            print(f\"\\nüîó **REFLECTION CHAIN:**\")\n",
    "            for j, reflection in enumerate(response.reflection_chain, 1):\n",
    "                print(f\"   {j}. {reflection}\")\n",
    "            \n",
    "            if response.utility_judgment.improvement_suggestions:\n",
    "                print(f\"\\nüí° **IMPROVEMENT SUGGESTIONS:**\")\n",
    "                for suggestion in response.utility_judgment.improvement_suggestions:\n",
    "                    print(f\"   ‚Ä¢ {suggestion}\")\n",
    "            \n",
    "            results.append({\n",
    "                'test_case': test_case['name'],\n",
    "                'success': True,\n",
    "                'retrieval_used': response.retrieval_decision.should_retrieve,\n",
    "                'quality_score': response.utility_judgment.utility_score,\n",
    "                'processing_time': response.processing_time\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR: {str(e)}\")\n",
    "            results.append({\n",
    "                'test_case': test_case['name'],\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            })\n",
    "        \n",
    "        print(f\"\\n{'üî∏' * 35} END TEST CASE {i} {'üî∏' * 35}\")\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    \n",
    "    # Generate comprehensive test report\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä SELF-RAG TEST REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    successful_tests = [r for r in results if r['success']]\n",
    "    \n",
    "    print(f\"\\nüéØ **OVERALL PERFORMANCE:**\")\n",
    "    print(f\"   ‚Ä¢ Tests Passed: {len(successful_tests)}/{len(results)} ({len(successful_tests)/len(results)*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Total Processing Time: {total_time:.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Avg Time per Query: {total_time/len(results):.2f}s\")\n",
    "    \n",
    "    if successful_tests:\n",
    "        retrieval_used = sum(1 for r in successful_tests if r.get('retrieval_used', False))\n",
    "        avg_quality = sum(r['quality_score'] for r in successful_tests) / len(successful_tests)\n",
    "        avg_time = sum(r['processing_time'] for r in successful_tests) / len(successful_tests)\n",
    "        \n",
    "        print(f\"\\nüìà **SELF-REFLECTION METRICS:**\")\n",
    "        print(f\"   ‚Ä¢ Retrieval Rate: {retrieval_used}/{len(successful_tests)} ({retrieval_used/len(successful_tests)*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Average Quality Score: {avg_quality:.3f}\")\n",
    "        print(f\"   ‚Ä¢ Average Processing Time: {avg_time:.3f}s\")\n",
    "    \n",
    "    # System statistics\n",
    "    stats = self_rag.get_system_statistics()\n",
    "    print(f\"\\nüèÜ **SYSTEM STATISTICS:**\")\n",
    "    print(f\"   ‚Ä¢ Total System Queries: {stats['total_queries']}\")\n",
    "    print(f\"   ‚Ä¢ Overall Retrieval Rate: {stats['retrieval_rate']:.1%}\")\n",
    "    print(f\"   ‚Ä¢ System Avg Quality: {stats['avg_quality_score']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ System Avg Time: {stats['avg_processing_time']:.3f}s\")\n",
    "    \n",
    "    print(f\"\\nüß† **SELF-RAG CAPABILITIES DEMONSTRATED:**\")\n",
    "    capabilities = [\n",
    "        \"‚úÖ Intelligent retrieval decision-making with [Retrieve] tokens\",\n",
    "        \"‚úÖ Relevance assessment with [ISREL] tokens\",\n",
    "        \"‚úÖ Support evaluation with [ISSUP] tokens\", \n",
    "        \"‚úÖ Utility judgment with [ISUSE] tokens\",\n",
    "        \"‚úÖ Self-awareness of knowledge limitations\",\n",
    "        \"‚úÖ Adaptive processing based on query complexity\",\n",
    "        \"‚úÖ Domain-specific confidence assessment\",\n",
    "        \"‚úÖ Transparent reflection chain tracking\"\n",
    "    ]\n",
    "    \n",
    "    for capability in capabilities:\n",
    "        print(f\"   {capability}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the comprehensive test suite\n",
    "test_results = run_self_rag_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f894d",
   "metadata": {},
   "source": [
    "## üéÆ Interactive Self-RAG Demo\n",
    "\n",
    "Experience Self-RAG with real-time self-reflection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48025ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_self_rag_demo():\n",
    "    print(\"\\n\" + \"üéÆ\" * 20 + \" SELF-RAG INTERACTIVE DEMO \" + \"üéÆ\" * 20)\n",
    "    print(\"üß† **SELF-REFLECTIVE AI ASSISTANT**\")\n",
    "    print(\"üéÆ\" * 60)\n",
    "    print(\"Experience AI that thinks about its own thinking! Watch how the system\")\n",
    "    print(\"decides when to retrieve information and evaluates its own responses.\")\n",
    "    print(\"\\nType 'quit' to exit, 'stats' for system statistics, 'help' for examples\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    demo_user = \"demo_user_selfrag\"\n",
    "    query_count = 0\n",
    "    \n",
    "    example_queries = {\n",
    "        \"Medical (High Retrieval)\": [\n",
    "            \"What are the latest COVID-19 treatments?\",\n",
    "            \"Explain diabetes management strategies\",\n",
    "            \"What is immunotherapy for cancer?\"\n",
    "        ],\n",
    "        \"Scientific (Complex)\": [\n",
    "            \"How does CRISPR gene editing work?\",\n",
    "            \"Analyze quantum entanglement applications\",\n",
    "            \"Explain the relationship between AI and scientific discovery\"\n",
    "        ],\n",
    "        \"Technology (Moderate)\": [\n",
    "            \"What is retrieval-augmented generation?\",\n",
    "            \"Compare different machine learning approaches\",\n",
    "            \"How do large language models work?\"\n",
    "        ],\n",
    "        \"Simple (Low Retrieval)\": [\n",
    "            \"What is AI?\",\n",
    "            \"Define machine learning\",\n",
    "            \"What is cloud computing?\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nüß† Your question for Self-RAG: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"\\nüëã Thank you for exploring Self-RAG!\")\n",
    "                print(\"üß† Remember: The best AI systems know when they don't know!\")\n",
    "                break\n",
    "            elif user_input.lower() == 'stats':\n",
    "                stats = self_rag.get_system_statistics()\n",
    "                print(f\"\\nüìä **SELF-RAG SYSTEM STATISTICS:**\")\n",
    "                print(f\"   ‚Ä¢ Total Queries Processed: {stats['total_queries']}\")\n",
    "                print(f\"   ‚Ä¢ Retrieval Decision Rate: {stats['retrieval_rate']:.1%}\")\n",
    "                print(f\"   ‚Ä¢ Average Quality Score: {stats['avg_quality_score']:.3f}\")\n",
    "                print(f\"   ‚Ä¢ Average Processing Time: {stats['avg_processing_time']:.3f}s\")\n",
    "                print(f\"   ‚Ä¢ Retrieve Decisions: {stats['retrieval_decisions']['retrieve']}\")\n",
    "                print(f\"   ‚Ä¢ No-Retrieve Decisions: {stats['retrieval_decisions']['no_retrieve']}\")\n",
    "                continue\n",
    "            elif user_input.lower() == 'help':\n",
    "                print(f\"\\nüí° **EXAMPLE QUERIES BY CATEGORY:**\")\n",
    "                for category, queries in example_queries.items():\n",
    "                    print(f\"\\nüî∏ **{category}:**\")\n",
    "                    for query in queries:\n",
    "                        print(f\"   ‚Ä¢ {query}\")\n",
    "                continue\n",
    "            elif not user_input:\n",
    "                print(\"Please enter a question or command.\")\n",
    "                continue\n",
    "            \n",
    "            query_count += 1\n",
    "            print(f\"\\nü§î Processing query #{query_count} with Self-RAG...\")\n",
    "            \n",
    "            response = self_rag.process_query(user_input, demo_user)\n",
    "            \n",
    "            print(f\"\\nü§ñ **SELF-RAG RESPONSE:**\")\n",
    "            print(f\"   {response.generated_answer}\")\n",
    "            \n",
    "            print(f\"\\nüß† **SELF-REFLECTION TOKENS:**\")\n",
    "            print(f\"   ‚Ä¢ {response.retrieval_decision.token.value}\")\n",
    "            if response.relevance_assessment:\n",
    "                print(f\"   ‚Ä¢ {response.relevance_assessment.token.value}\")\n",
    "            print(f\"   ‚Ä¢ {response.support_evaluation.token.value}\")\n",
    "            print(f\"   ‚Ä¢ {response.utility_judgment.token.value}\")\n",
    "            \n",
    "            print(f\"\\nüìä **PERFORMANCE METRICS:**\")\n",
    "            print(f\"   ‚Ä¢ Query Type: {response.query.complexity.value} {response.query.domain.value}\")\n",
    "            print(f\"   ‚Ä¢ Retrieval Used: {'Yes' if response.retrieval_decision.should_retrieve else 'No'}\")\n",
    "            print(f\"   ‚Ä¢ Documents Retrieved: {len(response.retrieved_documents)}\")\n",
    "            print(f\"   ‚Ä¢ Quality Score: {response.utility_judgment.utility_score:.2f}\")\n",
    "            print(f\"   ‚Ä¢ Processing Time: {response.processing_time:.2f}s\")\n",
    "            \n",
    "            print(f\"\\nüîó **REFLECTION REASONING:**\")\n",
    "            print(f\"   ‚Ä¢ Retrieval: {response.retrieval_decision.reasoning}\")\n",
    "            if response.relevance_assessment:\n",
    "                print(f\"   ‚Ä¢ Relevance: {response.relevance_assessment.reasoning}\")\n",
    "            print(f\"   ‚Ä¢ Support: {response.support_evaluation.reasoning}\")\n",
    "            print(f\"   ‚Ä¢ Utility: {response.utility_judgment.reasoning}\")\n",
    "            \n",
    "            if response.utility_judgment.improvement_suggestions:\n",
    "                print(f\"\\nüí° **SELF-IMPROVEMENT SUGGESTIONS:**\")\n",
    "                for suggestion in response.utility_judgment.improvement_suggestions:\n",
    "                    print(f\"   ‚Ä¢ {suggestion}\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüëã Demo interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "    \n",
    "    if query_count > 0:\n",
    "        print(f\"\\nüìà **DEMO SESSION SUMMARY:**\")\n",
    "        print(f\"   ‚Ä¢ Queries Processed: {query_count}\")\n",
    "        print(f\"   ‚Ä¢ User Profile: {demo_user}\")\n",
    "        print(f\"   ‚Ä¢ Self-Reflection Demonstrated: ‚úÖ\")\n",
    "\n",
    "print(\"\\nüí° **To start interactive Self-RAG demo, uncomment the next cell**\")\n",
    "print(\"üß† Experience AI that truly reflects on its own capabilities!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1822dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to start the interactive Self-RAG demo\n",
    "# interactive_self_rag_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a46b8-4f66-43a4-ab7b-d39c7276f3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
